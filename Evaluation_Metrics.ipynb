{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation_Metrics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3PUjVfsjWD4MWmbIBuNmY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palash04/Artificial-Intelligence/blob/master/Evaluation_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzfGjigKO1B0",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix - \n",
        "\n",
        "It is a table that is often used to describe the performance of a classification model. \n",
        "\n",
        "The goal is to describe the model performance from different angles other than the prediction accuracy. For example, suppose we are building a classifier to predict whether a patient is sick or healthy. The expected classifications are either positive as in the patient is sick, or negative, the patient is healthy. We run our model on 1,000 patients and enter the model predictions in the following table:\n",
        "\n",
        "![Screenshot 2020-08-27 at 10 17 13](https://user-images.githubusercontent.com/26361028/91385434-7b2f1e00-e84e-11ea-89bf-7913d2372336.png)\n",
        "\n",
        "Let's now define the most basic terms, which are whole numbers (not rates):\n",
        "\n",
        "- true positives (TP): These are cases in which the model has correctly predicted yes (they have the disease).\n",
        "- true negatives (TN): the model correctly predicted no disease.\n",
        "- false positives (FP): the model falsely predicted yes, but they don't actually have the disease (In some literature known as a \"Type I error\" or “error of the first kind”).\n",
        "- false negatives (FN): the model falsely predicted no, but they actually do have the disease (In some literature known as a \"Type II error\" or “error of the second kind”)\n",
        "\n",
        "Now, the patients that the model predict are negative (no disease) are the ones that the model believes that they are healthy and we can send them home without further care and the ones that we predict positive are the ones that we are going to send for further investigation. Now, which mistake would we rather make? Mistakenly diagnosing someone as positive (has disease) and sending them for more investigation is not as bad as mistakenly diagnosing someone as negative (healthy) and sending them home which could risk his life. The obvious choice of evaluation metric in this case is that we care more about the number of false negatives (FN). We want to find all the sick people, even if the model accidently classify more healthy people as sick. This metric is called Recall.\n",
        "\n",
        "## Precision and Recall - \n",
        "\n",
        "Recall (also known as Sensitivity): out of the sick patients, how many did we correctly diagnose as sick? In other words, how many times did the model mistakenly diagnose a positive patient as negative (i.e. false negatives FN)? Recall is calculated by the following equation:\n",
        "\n",
        "![Screenshot 2020-08-27 at 10 20 49](https://user-images.githubusercontent.com/26361028/91385626-fb558380-e84e-11ea-8b19-f7f4cb596759.png)\n",
        "\n",
        "\n",
        "Precision (also known as Specificity): it is the opposite of recall. It basically says: out of all the patients predicted to be positive, how many of them were actually positive (how many were actually sick)? In other words, what is the false positive rate of this model? Precision is calculated by the following equation:\n",
        "\n",
        "![Screenshot 2020-08-27 at 10 21 35](https://user-images.githubusercontent.com/26361028/91385677-17592500-e84f-11ea-87df-a75305d3de2b.png)\n",
        "\n",
        "It is important to call out that, while in the health diagnostic example we decided that recall is a better metric, other use cases would require different metrics like Precision. To be able to identify the most appropriate metric for your problem, ask yourself which of the two falses would you care about more, the false positive or the false negative? If FP, then you are looking for precision. If you care more about FN, then recall is your answer.\n",
        "\n",
        "Consider the spam email classifier for example, which of the two falses would you care about more? Falsely classifying a non-spam email as spam and it gets lost or falsely classifying a spam email as non-spam and it makes its way to the inbox folder? I believe you would care more about the former. You don’t want the receiver to lose an email because your model misclassified it as spam. We want to catch all spam BUT it is very bad to lose a non-spam email. In this example, precision is a suitable metric to use.\n",
        "\n",
        "In some applications, you might care about both precision and recall at the same time. That’s called F-Score\n",
        "\n",
        "## F-Score - \n",
        "In many cases, you would want to summarize the performance of the classifier with a single metric that represents both recall and precision. To do so, we can convert precision p and recall r into a single F-score metric. In mathematics, this is called the Harmonic Mean of p and r.\n",
        "\n",
        "![Screenshot 2020-08-27 at 10 24 14](https://user-images.githubusercontent.com/26361028/91385888-761e9e80-e84f-11ea-9327-84fba14775c0.png)\n",
        "\n",
        "\n",
        "The good thing about f-score is that it gives a good overall representation to how your model is doing. Let’s take a look at the health diagnostic example again. We agreed that this is a high recall model. But what if the model is doing really good on the FN and giving us a high recall score but it is performing really poor on the FP and giving us a low precision score. Doing poor on FP means, that in order to not miss any sick patient, it’s gone so far on the other side to mistakenly diagnosing a lot of patients as sick to be on the safe side. So, while recall might be more important for this problem, it is good to look at the model from both scores precision and recall together.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzRdkAwzOs4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}