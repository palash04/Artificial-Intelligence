{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_LSTM_from_Scratch_In_Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S4IA1-raOZ3"
      },
      "source": [
        "# LSTM\n",
        "![LSTM](https://user-images.githubusercontent.com/26361028/125639171-b7f92866-2481-4293-b9ad-af85281c0967.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YInM6TDai9V"
      },
      "source": [
        "$\\textbf{F}_t = \\sigma(\\textbf{X}_t\\textbf{W}_{xf} + \\textbf{H}_{t-1}\\textbf{W}_{hf} + \\textbf{b}_f)$\n",
        "\n",
        "$\\textbf{I}_t = \\sigma(\\textbf{X}_t\\textbf{W}_{xi} + \\textbf{H}_{t-1}\\textbf{W}_{hi} + \\textbf{b}_i)$\n",
        "\n",
        "$\\textbf{O}_t = \\sigma(\\textbf{X}_t\\textbf{W}_{xo} + \\textbf{H}_{t-1}\\textbf{W}_{ho} + \\textbf{b}_o)$\n",
        "\n",
        "where, \n",
        "\n",
        "$\\textbf{W}_{xf}$, $\\textbf{W}_{xi}$, $\\textbf{W}_{xo}$ $\\in \\mathbb{R}^{dxh}$\n",
        "\n",
        "$\\textbf{W}_{hf}$, $\\textbf{W}_{hi}$, $\\textbf{W}_{ho}$ $\\in \\mathbb{R}^{hxh}$\n",
        "\n",
        "$\\textbf{b}_{f}$, $\\textbf{b}_{i}$, $\\textbf{b}_{o}$ $\\in \\mathbb{R}^{1xh}$\n",
        "\n",
        "**Candidate Memory Cell**:\n",
        "\n",
        "$\\tilde{\\textbf{C}_t} = tanh(\\textbf{X}_t\\textbf{W}_{xc} + \\textbf{H}_{t-1}\\textbf{W}_{hc} + \\textbf{b}_c)$\n",
        "\n",
        "where,\n",
        "\n",
        "$\\textbf{W}_{xc} \\in \\mathbb{R}^{dxh}$\n",
        "\n",
        "$\\textbf{W}_{hc} \\in \\mathbb{R}^{hxh}$\n",
        "\n",
        "$\\textbf{b}_{c} \\in \\mathbb{R}^{1xh}$\n",
        "\n",
        "**Memory Cell**:\n",
        "\n",
        "$\\textbf{C}_t = \\textbf{F}_t \\odot \\textbf{C}_{t-1} + \\textbf{I}_t \\odot \\tilde{\\textbf{C}_t}$\n",
        "\n",
        "**Hidden Cell**:\n",
        "\n",
        "$\\textbf{H}_t = \\textbf{O}_t \\odot tanh(\\textbf{C}_t)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND2AHtc6uXGe"
      },
      "source": [
        "!pip install d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dMBlwR1aGzP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from d2l import torch as d2l"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54GClj_kueeF",
        "outputId": "7f25d606-1e58-4057-f865-3967b3cc9bcf"
      },
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qBkPCbGuoie"
      },
      "source": [
        "# Initializing Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBAknO9QumoP"
      },
      "source": [
        "def get_lstm_params(vocab_size, num_hiddens, device):\n",
        "  num_inptus = num_outputs = vocab_size\n",
        "\n",
        "  def normal(shape):\n",
        "    return torch.randn(size=shape, device=device) * 0.01\n",
        "  \n",
        "  def three():\n",
        "    return (normal((num_inputs, num_hiddens)),\n",
        "            normal((num_hiddens, num_hiddens)),\n",
        "            torch.zeros(num_hiddens, device=device))\n",
        "    \n",
        "  W_xi, W_hi, b_i = three() # Input gate parameters\n",
        "  W_xf, W_hf, b_f = three() # Forget gate parameters\n",
        "  W_xo, W_ho, b_o = three() # Output gate parameters\n",
        "  W_xc, W_hc, b_c = three() # Candidate memory cell parameters\n",
        "\n",
        "  # Output parameters\n",
        "  W_hy = normal((num_hiddens, num_outputs))\n",
        "  b_y = torch.zeros(num_outputs, device=device)\n",
        "\n",
        "  # Attach gradients\n",
        "  params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hy, b_y]\n",
        "  for param in params:\n",
        "    param.requires_grad_(True)\n",
        "\n",
        "  return params"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPpGYP5uwQow"
      },
      "source": [
        "# Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0yru62xwO5l"
      },
      "source": [
        "def init_lstm_state(batch_size, num_hiddens, device):\n",
        "  return (torch.zeros((batch_size, num_hiddens), device=device), \n",
        "          torch.zeros((batch_size, num_hiddens), device=device))\n",
        "  \n",
        "def lstm(inputs, state, params):\n",
        "  # inputs shape: (num_steps, batch_size, vocab_size)\n",
        "  W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hy, b_y = params\n",
        "  (H,C) = state\n",
        "  outputs = []\n",
        "\n",
        "  for X in inputs:\n",
        "    # X shape: (batch_size, vocab_size)\n",
        "    I = torch.sigmoid( (X @ W_xi) + (H @ W_hi) + b_i)\n",
        "    F = torch.sigmoid( (X @ W_xf) + (H @ W_hf) + b_f)\n",
        "    O = torch.sigmoid( (X @ W_xo) + (H @ W_ho) + b_o)\n",
        "\n",
        "    C_tilda = torch.tanh( (X @ W_xc) + (H @ W_hc) + b_c)\n",
        "\n",
        "    C = F * C + I * C_tilda\n",
        "    H = O * torch.tanh(C)\n",
        "    Y = (H @ W_hy) + b_y    # shape: (batch_size, vocab_size)\n",
        "    outputs.append(Y)\n",
        "\n",
        "  return torch.cat(outputs, dim=0), (H,C) # shape: (num_steps x batch_size, vocab_size), (H,C)=> (n x h) each"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jqUrU3Ex5GL"
      },
      "source": [
        "# Training and prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3pfs5_xwnML"
      },
      "source": [
        "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
        "num_epochs, lr = 500, 1\n",
        "model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,\n",
        "                            init_lstm_state, lstm)\n",
        "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw8nLhPRyhej"
      },
      "source": [
        "# Concise Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSN99FryhOU"
      },
      "source": [
        "num_inputs = vocab_size\n",
        "lstm_layer = nn.LSTM(num_inputs, num_hiddens)\n",
        "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
        "model = model.to(device)\n",
        "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}