{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_RNN_From_Scratch_In_Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIjmwwPXXhy"
      },
      "source": [
        "Following implementation makes use of pytorch's computational graph to do backprop, else everything is from sratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0divWw1OkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f53ab64-0e97-4994-aa05-2289861a254d"
      },
      "source": [
        "!pip install d2l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.16.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFj2KkkY1aJc"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from d2l import torch as d2l\n",
        "import re\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMkuqAYl4PV9"
      },
      "source": [
        "Reading the data\n",
        "\n",
        "Data taken from \n",
        "[H. G. Wellsâ€™ The Time Machine](http://www.gutenberg.org/ebooks/35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLUEZZdzXvG-"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVkUzCAJ9mQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0151a892-381d-4c88-c0e1-343959d3a36d"
      },
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5IzLtsOYDHI",
        "outputId": "17ba5626-8fa3-4fa1-cf98-6d30c5df1b3b"
      },
      "source": [
        "print(len(vocab))\n",
        "print(vocab.token_to_idx)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n",
            "{'<unk>': 0, ' ': 1, 'e': 2, 't': 3, 'a': 4, 'i': 5, 'n': 6, 'o': 7, 's': 8, 'h': 9, 'r': 10, 'd': 11, 'l': 12, 'm': 13, 'u': 14, 'c': 15, 'f': 16, 'w': 17, 'g': 18, 'y': 19, 'p': 20, 'b': 21, 'v': 22, 'k': 23, 'x': 24, 'z': 25, 'j': 26, 'q': 27}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFu6YvoLYgd-",
        "outputId": "955dfdf6-07ca-434b-9185-3283f2e721a7"
      },
      "source": [
        "x,y = next(iter(train_iter))\n",
        "print(x.shape) # (batch_size x num_steps)\n",
        "print(y.shape) # (batch_size x num_steps)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 35])\n",
            "torch.Size([32, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s61Skg_PYnNK",
        "outputId": "67002d7b-2fdb-4b21-c80e-0190ffcbe9f2"
      },
      "source": [
        "print(x[0])\n",
        "print(y[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 5,  3,  9,  2,  1,  3,  5, 13,  2,  1,  3, 10,  4, 22,  2, 12, 12,  2,\n",
            "        10,  1, 16,  7, 10,  1,  8,  7,  1,  5,  3,  1, 17,  5, 12, 12,  1])\n",
            "tensor([ 3,  9,  2,  1,  3,  5, 13,  2,  1,  3, 10,  4, 22,  2, 12, 12,  2, 10,\n",
            "         1, 16,  7, 10,  1,  8,  7,  1,  5,  3,  1, 17,  5, 12, 12,  1, 21])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz2xrmUmYtj3",
        "outputId": "a51c1337-c274-4e91-90d0-9aaed1de9047"
      },
      "source": [
        "# One hot encoding\n",
        "\n",
        "# input of shape (batch_size x num_steps)\n",
        "X = torch.arange(10).reshape((2,5))\n",
        "\n",
        "# we want (num_steps x batch_size x len(vocab))\n",
        "\n",
        "F.one_hot(X.T, len(vocab)).shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebte02OfhTll"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZlOfzfyZZW0"
      },
      "source": [
        "# Initializing the model parameters\n",
        "\n",
        "The number of hidden units num_hiddens is a tunable hyperparameter. When training language models, the inputs and outputs are from the same vocabulary. Hence, they have the same dimension, which is equal to the vocabulary size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piX1bahIaRTl"
      },
      "source": [
        "Mini batch of examples\n",
        "$\\textbf{X}_{t} \\in \\mathbb{R}^{nxd}$, where n = batch_size, d = inputs size (vocab_size) at time step t. Each row of $\\textbf{X}_t$ corresponds to one example at time step t. \n",
        "\n",
        "Hidden layer's output at time $t-1$,\n",
        "$\\textbf{H}_{t-1} \\in \\mathbb{R}^{nxh}$, where n = batch_size, h = number of hidden units in hidden layer from previous time step.\n",
        "\n",
        "Hidden variable of current time step $t$ is calculated as \n",
        "\n",
        "$\\textbf{H}_t = \\phi(\\textbf{X}_t\\textbf{W}_{xh} + \\textbf{H}_{t-1}\\textbf{W}_{hh} + \\textbf{b}_h)$\n",
        "\n",
        "where, \n",
        "- $\\textbf{W}_{xh} \\in \\mathbb{R}^{dxh}$ \n",
        "- $\\textbf{W}_{hh} \\in \\mathbb{R}^{hxh}$ \n",
        "- $\\textbf{b}_h \\in \\mathbb{R}^{1xh}$\n",
        "\n",
        "Output layer at time $t$ is calculated as\n",
        "\n",
        "$\\textbf{O}_t = \\textbf{H}_t\\textbf{W}_{hy} + \\textbf{b}_y$\n",
        "\n",
        "where, $\\textbf{W}_{hy} \\in \\mathbb{R}^{hxy}$ and,\n",
        "$\\textbf{b}_h \\in \\mathbb{R}^{1xy}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpTruIwDZOWm"
      },
      "source": [
        "def get_params(vocab_size, num_hiddens, device):\n",
        "  num_inputs = num_outputs = vocab_size\n",
        "\n",
        "  def normal(shape):\n",
        "    return torch.randn(size=shape, device=device) * 0.01\n",
        "  \n",
        "  # Hidden layer parameters\n",
        "  W_xh = normal((num_inputs, num_hiddens))\n",
        "  W_hh = normal((num_hiddens, num_hiddens))\n",
        "  b_h = torch.zeros(num_hiddens, device=device)\n",
        "\n",
        "  # Output layer parameters\n",
        "  W_hy = normal((num_hiddens, num_outputs))\n",
        "  b_y = torch.zeros(num_outputs, device=device)\n",
        "\n",
        "  # Attach gradients\n",
        "  params = [W_xh, W_hh, b_h, W_hy, b_y]\n",
        "  for param in params:\n",
        "    param.requires_grad_(True)\n",
        "\n",
        "  return params"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwxybiuOfi9x"
      },
      "source": [
        "# RNN Model\n",
        "\n",
        "To define an RNN model, we first need an init_rnn_state function to return the hidden state at initialization. It returns a tensor filled with 0 and with a shape of (batch size, number of hidden units). Using tuples makes it easier to handle situations where the hidden state contains multiple variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZqcFgxzfhMp"
      },
      "source": [
        "def init_rnn_state(batch_size, num_hiddens, device):\n",
        "  return (torch.zeros((batch_size, num_hiddens), device=device) ,)\n",
        "\n",
        "def rnn(inputs, state, params):\n",
        "  # inputs shape: (num_steps, batch_size, vocab_size)\n",
        "  W_xh, W_hh, b_h, W_hy, b_y = params\n",
        "  H, = state\n",
        "  outputs = []\n",
        "  for X in inputs:\n",
        "    # shape of X: (batch_size, vocab_size)\n",
        "    # b_h and b_y are broadcasted\n",
        "    H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
        "    Y = torch.mm(H, W_hy) + b_y # (batch_size x vocab_size)\n",
        "    outputs.append(Y)\n",
        "  return torch.cat(outputs, dim=0), (H,)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUH351NTf6bY",
        "outputId": "6c86627b-78a7-4478-d589-56c72fdd118f"
      },
      "source": [
        "# Testing above function\n",
        "num_hiddens = 256\n",
        "params = get_params(len(vocab), num_hiddens=num_hiddens, device=device)\n",
        "state = init_rnn_state(batch_size=batch_size, num_hiddens=num_hiddens, device=device)\n",
        "inputs = torch.rand(size=(num_steps, batch_size, len(vocab))).to(device)\n",
        "output, state = rnn(inputs, state, params)\n",
        "print(output.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1120, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1-zaY74hPH9"
      },
      "source": [
        "class RNNModelScratch():\n",
        "  def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
        "    # forward_fn: can be forward implementation of rnn or gru or lstm\n",
        "    self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "    self.params = get_params(vocab_size, num_hiddens, device)\n",
        "    self.init_state, self.forward_fn = init_state, forward_fn\n",
        "  \n",
        "  def __call__(self, X, state):\n",
        "    # Here X: (batch_size x num_steps)\n",
        "    X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "    # Shape of X: (num_steps, batch_size, vocab_size)\n",
        "    return self.forward_fn(X, state, self.params)\n",
        "  \n",
        "  def begin_state(self, batch_size, device):\n",
        "    return self.init_state(batch_size, self.num_hiddens, device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlG_iuVUjsBW",
        "outputId": "0349262a-6b76-47d8-8ed0-b86e4b2e878c"
      },
      "source": [
        "# batch_size x num_steps\n",
        "X = torch.arange(10).reshape((2,5))\n",
        "\n",
        "num_hiddens = 512\n",
        "net = RNNModelScratch(vocab_size=len(vocab), num_hiddens=num_hiddens, device=device, get_params=get_params, init_state=init_rnn_state, forward_fn=rnn)\n",
        "state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "Y, new_state = net(X.to(device), state)\n",
        "print(Y.shape) # (num_steps x batch_size, vocab_size)\n",
        "print(len(new_state))\n",
        "print(new_state[0].shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 28])\n",
            "1\n",
            "torch.Size([2, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T1CMiP-k4SL"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSDXf1ZdkN3_"
      },
      "source": [
        "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
        "  \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "  state = net.begin_state(batch_size=1, device=device) # (batch_size x h)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(\n",
        "      (1, 1))\n",
        "  for y in prefix[1:]:  # Warm-up period\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y])\n",
        "  for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "    y, state = net(get_input(), state)\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4HlvOHRrlH2J",
        "outputId": "9bfce4b1-d3a1-469f-923b-b80a4ddc4afc"
      },
      "source": [
        "predict_ch8('time traveller ', 10, net, vocab, device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'time traveller jlcqxenlcq'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4THdzAPglM9c"
      },
      "source": [
        "# Gradient Clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87KbI1-8lJUQ"
      },
      "source": [
        "def grad_clipping(net, theta):\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  if isinstance(net, nn.Module):\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "  else:\n",
        "    params = net.params\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
        "  if norm > theta:\n",
        "    for param in params:\n",
        "      param.grad[:] *= theta / norm"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urQ7SFcsloAd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy0lcjAPlR6Y"
      },
      "source": [
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "  for X, Y in train_iter:\n",
        "    # X: (batch_size x num_steps)\n",
        "    # Y: (batch_size x num_steps)\n",
        "    state = None\n",
        "    if state is None or use_random_iter:\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "        # state is a tensor for nn.GRU\n",
        "        state.detach_()\n",
        "      else:\n",
        "        for s in state:\n",
        "          s.detach_()\n",
        "    \n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "      updater.zero_grad()\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater.step()\n",
        "    else:\n",
        "      l.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      updater(batch_size=1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG_B0DbUnbJW"
      },
      "source": [
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  if isinstance(net, nn.Module):\n",
        "    updater = torch.optim.SGD(net.parameters(), lr)\n",
        "  else:\n",
        "    updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
        "  \n",
        "  predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "\n",
        "  # Train and predict\n",
        "  for epoch in range(num_epochs):\n",
        "    train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(f'Epoch: {epoch+1}')\n",
        "      print(predict('time traveller'))\n",
        "    \n",
        "  print(predict('time traveller'))\n",
        "  print(predict('traveller'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VMbRSkzoLkG",
        "outputId": "2c9adf22-673b-48fd-e266-be812a0a3de1"
      },
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10\n",
            "time traveller and and the the thate the thate the thate the tha\n",
            "Epoch: 20\n",
            "time travellere the the the the the the the the the the the the \n",
            "Epoch: 30\n",
            "time traveller the gratin this this this thith sime that s and t\n",
            "Epoch: 40\n",
            "time traveller the pace this the proment dimensions and the thin\n",
            "Epoch: 50\n",
            "time travellertour and and the this treeredinet mavely that said\n",
            "Epoch: 60\n",
            "time traveller the prome tore ware this thing sion sime thave ti\n",
            "Epoch: 70\n",
            "time travellerit t angithe mave at in thisgis tr all ghour and t\n",
            "Epoch: 80\n",
            "time traveller the pace theegraveledraccedend sur and have it is\n",
            "Epoch: 90\n",
            "time traveller tho ghat s ane her about the room and that sis th\n",
            "Epoch: 100\n",
            "time traveller th cereally boct thithing a of thee this four dim\n",
            "Epoch: 110\n",
            "time traveller but howhis the forme athing difer man as i chenot\n",
            "Epoch: 120\n",
            "time traveller ffor ano hime and this time thaveller ffor any hi\n",
            "Epoch: 130\n",
            "time traveller ffreed his psere bling bof comistencefilby bucal \n",
            "Epoch: 140\n",
            "time traveller of thewe the itit toree an this but expsand thave\n",
            "Epoch: 150\n",
            "time traveller held anocing is reat difcind is ferty un aly hang\n",
            "Epoch: 160\n",
            "time traveller hald inyes ngt in this that disterenttan there as\n",
            "Epoch: 170\n",
            "time traveller argee thit soumhas ix wisuralarge there are gareo\n",
            "Epoch: 180\n",
            "time traveller of enthan thene are bace this to serace or havene\n",
            "Epoch: 190\n",
            "time travellerites again time tsat y ureaw onime that d as a sai\n",
            "Epoch: 200\n",
            "time travellerit s against reason thind that noth rsatdet lysmoo\n",
            "Epoch: 210\n",
            "time travellerit s against reasonas it spaness mestenction at ri\n",
            "Epoch: 220\n",
            "time travellerit s against reason said the medical man there are\n",
            "Epoch: 230\n",
            "time traveller held in his hand was a glitteringmetallic framewo\n",
            "Epoch: 240\n",
            "time traveller proceeded anyreal body must have extension in fou\n",
            "Epoch: 250\n",
            "time travellerit s against reason said the medical man staring h\n",
            "Epoch: 260\n",
            "time travellerit s against reason said filbycan a cube that dies\n",
            "Epoch: 270\n",
            "time travellerit s against reason said the medical man there are\n",
            "Epoch: 280\n",
            "time travellerit s against reason said filbycan a cube that does\n",
            "Epoch: 290\n",
            "time travellerit s againet in a frams ceat tory a dimensions and\n",
            "Epoch: 300\n",
            "time travellerit s against reason said filbycan a cube that does\n",
            "Epoch: 310\n",
            "time travellerit s against reason said the medical man andfit yo\n",
            "Epoch: 320\n",
            "time traveller proceeded anyreal body must have extension in fou\n",
            "Epoch: 330\n",
            "time traveller held in his hand was a glitteringmetallic framewo\n",
            "Epoch: 340\n",
            "time travellerit s against reason said filby an a gume that abe \n",
            "Epoch: 350\n",
            "time traveller trmeet tho ghit s all humbug you knowthe time tra\n",
            "Epoch: 360\n",
            "time travellerit s against reason said filbycan a cube that does\n",
            "Epoch: 370\n",
            "time traveller held in his hand was a glitteringmetallic framewo\n",
            "Epoch: 380\n",
            "time travellerit s against reason said filbycan a cube that does\n",
            "Epoch: 390\n",
            "time travellerit s against reason said filby an a gume tract att\n",
            "Epoch: 400\n",
            "time travellerit s against reason said filbycan a cube that does\n",
            "Epoch: 410\n",
            "time travellerit s against reason said filby of course a solid b\n",
            "Epoch: 420\n",
            "time travellerit s against reason said filby an a giner has amat\n",
            "Epoch: 430\n",
            "time travellerit s against reason said filby of course a solid b\n",
            "Epoch: 440\n",
            "time traveller but now you begin to seethe object of my investig\n",
            "Epoch: 450\n",
            "time travellerit s against reason said filby an argumentative pe\n",
            "Epoch: 460\n",
            "time travellerit s against reason said filbywhat is ala recoldit\n",
            "Epoch: 470\n",
            "time travellerit s against reason said filbywhur in the overe we\n",
            "Epoch: 480\n",
            "time travellerit s against reason said filbywhat said a very you\n",
            "Epoch: 490\n",
            "time travellerit s against reason said filbywhat llane thisgeade\n",
            "Epoch: 500\n",
            "time travellerit s against reason said filby of course a solid b\n",
            "time travellerit s against reason said filby of course a solid b\n",
            "travellerit s against reason said filby of course a solid b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOoBOhrhoM4l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}