{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Character_Level_RNN_Using_PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_0vP00Wg7X9"
      },
      "source": [
        "Character level RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JciyftVtm0Th"
      },
      "source": [
        "!pip install d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNim5i_m9rU"
      },
      "source": [
        "import math\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-49v-jJynIPU",
        "outputId": "a56b4b5c-0d55-41a5-9bbd-454e9dbc0347"
      },
      "source": [
        "# Load data\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HjtGFIGniWk",
        "outputId": "70551918-2370-4381-a35b-b0edac248485"
      },
      "source": [
        "print(len(vocab))\n",
        "print(vocab.token_freqs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n",
            "[(' ', 29927), ('e', 17838), ('t', 13515), ('a', 11704), ('i', 10138), ('n', 9917), ('o', 9758), ('s', 8486), ('h', 8257), ('r', 7674), ('d', 6337), ('l', 6146), ('m', 4043), ('u', 3805), ('c', 3424), ('f', 3354), ('w', 3225), ('g', 3075), ('y', 2679), ('p', 2427), ('b', 1897), ('v', 1295), ('k', 1087), ('x', 236), ('z', 144), ('j', 97), ('q', 95)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZP0_T7hnl1m",
        "outputId": "4ce8b457-bca2-43dc-f40e-62a2c2297771"
      },
      "source": [
        "x, y = next(iter(train_iter))\n",
        "print(x.shape, y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 35]) torch.Size([32, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6JNM5ExnurS",
        "outputId": "16885525-1fe7-425a-eba3-bd40d99979a9"
      },
      "source": [
        "print(x[0])\n",
        "print(y[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([21, 19,  1,  9,  1, 18,  1, 17,  2, 12, 12,  8,  5,  3,  9,  2,  1,  3,\n",
            "         5, 13,  2,  1,  3, 10,  4, 22,  2, 12, 12,  2, 10,  1, 16,  7, 10])\n",
            "tensor([19,  1,  9,  1, 18,  1, 17,  2, 12, 12,  8,  5,  3,  9,  2,  1,  3,  5,\n",
            "        13,  2,  1,  3, 10,  4, 22,  2, 12, 12,  2, 10,  1, 16,  7, 10,  1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT8TjKTXhKaQ"
      },
      "source": [
        "One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yEmxihny7F",
        "outputId": "6aebc3a6-6f17-4af6-f7f7-0a5a080e59cd"
      },
      "source": [
        "F.one_hot(torch.tensor([0,3,5]), len(vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfo5HLZghsOo"
      },
      "source": [
        "The shape of the minibatch that we sample each time is (batch size, number of time steps). The one_hot function transforms such a minibatch into a three-dimensional tensor with the last dimension equals to the vocabulary size (len(vocab)). We often transpose the input so that we will obtain an output of shape (number of time steps, batch size, vocabulary size). This will allow us to more conveniently loop through the outermost dimension for updating hidden states of a minibatch, time step by time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdOfP5l-hQSA",
        "outputId": "80f21c84-476c-4da2-ae2c-5fa741346e89"
      },
      "source": [
        "X = torch.arange(10).reshape(2,5)\n",
        "F.one_hot(X.T, len(vocab)).shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC9HfsHW0VYn"
      },
      "source": [
        "# Defining the Model\n",
        "\n",
        "RNN Model\n",
        "\n",
        "[torch.nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "\n",
        "Parameters:\n",
        "- input feature size in x\n",
        "- number of features in hidden state h\n",
        "- number of recurrent layers\n",
        "- batch_first: if **True** then input and output shape: (batch_size, seq_len, feature); if **False** then input and output shape: (seq_len, batch_size, feature)\n",
        "- bidirectional: True/False\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXZNhoTRmF3b"
      },
      "source": [
        "num_hiddens = 256\n",
        "num_layers = 1\n",
        "rnn_layer = nn.RNN(input_size=len(vocab), hidden_size=num_hiddens, num_layers=num_layers, batch_first=False)\n",
        "# rnn_layer = nn.RNN(input_size=len(vocab), hidden_size=num_hiddens, num_layers=num_layers, batch_first=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51OvDf5R1_Sc"
      },
      "source": [
        "Shape of hidden state: (D*num_layers, batch_size, hidden_size)\n",
        "\n",
        "where, D = 2 if bidirectional = True otherwise 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGVC_UrHw3-L",
        "outputId": "dd80ea18-0598-42ff-b54d-e9335027b660"
      },
      "source": [
        "# initialize the hidden state h0\n",
        "state = torch.zeros((num_layers, batch_size, num_hiddens))\n",
        "state.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcZn0ii-2Z-O"
      },
      "source": [
        "Computing hidden state\n",
        "\n",
        "\n",
        "Input shape:\n",
        "```\n",
        "  if batch_first is False:\n",
        "    (seq_len, batch_size, feature_size)\n",
        "  else:\n",
        "    (batch_size, seq_len, feature_size)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fTAHS3e2UG8",
        "outputId": "f2b7f5d4-d3f9-4781-b984-d9f9f414b4c9"
      },
      "source": [
        "# If batch_first = True\n",
        "# X = torch.rand(size = (batch_size, num_steps, len(vocab)))\n",
        "# Y, state_new = rnn_layer(X, state)  \n",
        "# print(Y.shape)  # (batch_size, num_steps, num_hiddens)\n",
        "# print(state_new.shape) # (num_layers, batch_size, num_hiddens)\n",
        "\n",
        "# If batch_first = False\n",
        "X = torch.rand(size = (num_steps, batch_size, len(vocab)))\n",
        "Y, state_new = rnn_layer(X, state)\n",
        "print(Y.shape)\n",
        "print(state_new.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([35, 32, 256])\n",
            "torch.Size([1, 32, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doZY-COw3Jh6"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "    super(RNN, self).__init__(**kwargs)\n",
        "    self.rnn = rnn_layer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_hiddens = self.rnn.hidden_size\n",
        "\n",
        "    if not self.rnn.bidirectional:\n",
        "      self.num_directions = 1\n",
        "      self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "    else:\n",
        "      self.num_directions = 2\n",
        "      self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    # inputs shape: (batch_size x num_steps)\n",
        "    # since batch first is false we transpose inputs\n",
        "    # one hot encode\n",
        "    X = F.one_hot(inputs.T.long(), self.vocab_size) \n",
        "    # now X has shape (num_steps x batch_size x vocab_size)\n",
        "    X = X.to(torch.float32)\n",
        "    Y, state = self.rnn(X, state)\n",
        "    # shape of Y: (num_steps x batch_size x num_hiddens)\n",
        "\n",
        "    # fully connected layer will change shape of Y to \n",
        "    # (num_steps x batch_size x vocab_size)\n",
        "    output = self.linear(Y.reshape(-1, Y.shape[-1]))\n",
        "    return output, state\n",
        "  \n",
        "  def begin_state(self, device, batch_size=1):\n",
        "    if not isinstance(self.rnn, nn.LSTM):\n",
        "      # nn.GRU takes a tensor as hidden state\n",
        "      return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)\n",
        "    else:\n",
        "      # nn.LSTM takes a tuple of hidden states\n",
        "      return (torch.zeros((\n",
        "                  self.num_directions * self.rnn.num_layers, \n",
        "                  batch_size, \n",
        "                  self.num_hiddens), \n",
        "              device = device), \n",
        "              torch.zeros((\n",
        "                  self.num_directions * self.rnn.num_layers, \n",
        "                  batch_size, \n",
        "                  self.num_hiddens), \n",
        "               device = device))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59EjxO6KUp3V",
        "outputId": "9845e660-001f-4125-898a-51c0fe4e0de6"
      },
      "source": [
        "# testing rnn architecture\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "rnn_layer = nn.RNN(len(vocab), num_hiddens, num_layers)\n",
        "net = RNN(rnn_layer, len(vocab)).to(device)\n",
        "inputs = torch.rand(1,1, device=device)\n",
        "state = net.begin_state(batch_size=1, device=device)\n",
        "y, _ = net(inputs, state)\n",
        "y.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AeGnzPQVkGG",
        "outputId": "7dac2e57-ed36-4712-eab2-9c6cb1815ee4"
      },
      "source": [
        "# maximum value occuring at index \n",
        "int(y.argmax(dim=1).reshape(1))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57F-ChcpOQce"
      },
      "source": [
        "# Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2zF5qmhMiZ_"
      },
      "source": [
        "def predict(prefix, num_preds, net, vocab, device):\n",
        "  # prefix: user provided text to start with\n",
        "  # num_preds: the number of characters to be predicted\n",
        "  # the rnn net object\n",
        "  # vocab: dictionary mapping character to idx\n",
        "  \n",
        "  state = net.begin_state(device, batch_size=1)\n",
        "  outputs = [vocab[prefix[0]]]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(1,1)\n",
        "\n",
        "  # warm-up period: model updates itself but does not make predictions\n",
        "  for y in prefix[1:]:\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y])\n",
        "  \n",
        "  # predict 'num_preds' step\n",
        "  for _ in range(num_preds):\n",
        "    y, state = net(get_input(), state)\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c69NfIgUWYhZ",
        "outputId": "20bf1810-0b02-4faf-f9da-b1a7c6ac7eeb"
      },
      "source": [
        "predict('time traveller ', num_preds=10, net=net, vocab=vocab, device=device)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'time traveller oaoaoaoaoa'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkCfIxz8aXR3"
      },
      "source": [
        "Gradient clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14lsVQTZaVmE"
      },
      "source": [
        "def grad_clipping(net, theta):\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  if isinstance(net, nn.Module):\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "  else:\n",
        "    params = net.params\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
        "  if norm > theta:\n",
        "      for param in params:\n",
        "          param.grad[:] *= theta / norm"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "668TmeR1WuT5"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnrs7lRBWiTM"
      },
      "source": [
        "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "  pred = lambda prefix: predict(prefix, 50, net, vocab, device)\n",
        "\n",
        "  state = None\n",
        "  # train and predict\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    cnt = 0\n",
        "    for X, y in train_iter:\n",
        "      cnt += 1\n",
        "      # shape of X: (batch_size x num_steps)\n",
        "      # shape of y: (batch_size x num_steps)\n",
        "      if state is None:\n",
        "        state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "      else:\n",
        "        if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "          # state is a tensor for nn.FRU\n",
        "          state.detach_()\n",
        "        else:\n",
        "          # state is a tuple of tensors for nn.LSTM\n",
        "          for s in state:\n",
        "            s.detach_()\n",
        "      y = y.T.reshape(-1)\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_hat, state = net(X, state)\n",
        "      \n",
        "      loss = criterion(y_hat, y.long()).mean()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      grad_clipping(net, 1)\n",
        "      optimizer.step()\n",
        "      running_loss += loss\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print('****')\n",
        "      print(pred('time traveller '))\n",
        "      print('****')\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs}\\tLoss: {running_loss/cnt:.6f}')\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNMlfrsnWrcl",
        "outputId": "d0b0a99e-ad5a-478e-f4de-6eaa7852f41f"
      },
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train(net, train_iter, vocab, lr, num_epochs, device)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/500\tLoss: 3.097515\n",
            "Epoch: 2/500\tLoss: 2.921096\n",
            "Epoch: 3/500\tLoss: 2.813699\n",
            "Epoch: 4/500\tLoss: 2.756021\n",
            "Epoch: 5/500\tLoss: 2.680970\n",
            "Epoch: 6/500\tLoss: 2.606729\n",
            "Epoch: 7/500\tLoss: 2.543646\n",
            "Epoch: 8/500\tLoss: 2.501996\n",
            "Epoch: 9/500\tLoss: 2.459864\n",
            "****\n",
            "time traveller the the the the the the the the the the the the th\n",
            "****\n",
            "Epoch: 10/500\tLoss: 2.411392\n",
            "Epoch: 11/500\tLoss: 2.394263\n",
            "Epoch: 12/500\tLoss: 2.370791\n",
            "Epoch: 13/500\tLoss: 2.336989\n",
            "Epoch: 14/500\tLoss: 2.320216\n",
            "Epoch: 15/500\tLoss: 2.309089\n",
            "Epoch: 16/500\tLoss: 2.278907\n",
            "Epoch: 17/500\tLoss: 2.268442\n",
            "Epoch: 18/500\tLoss: 2.250309\n",
            "Epoch: 19/500\tLoss: 2.257007\n",
            "****\n",
            "time traveller the the the the the the the the the the the the th\n",
            "****\n",
            "Epoch: 20/500\tLoss: 2.228314\n",
            "Epoch: 21/500\tLoss: 2.221901\n",
            "Epoch: 22/500\tLoss: 2.203267\n",
            "Epoch: 23/500\tLoss: 2.185552\n",
            "Epoch: 24/500\tLoss: 2.188191\n",
            "Epoch: 25/500\tLoss: 2.182741\n",
            "Epoch: 26/500\tLoss: 2.144275\n",
            "Epoch: 27/500\tLoss: 2.157930\n",
            "Epoch: 28/500\tLoss: 2.139886\n",
            "Epoch: 29/500\tLoss: 2.142646\n",
            "****\n",
            "time traveller andithe andithe andithe andithe andithe andithe an\n",
            "****\n",
            "Epoch: 30/500\tLoss: 2.131577\n",
            "Epoch: 31/500\tLoss: 2.125254\n",
            "Epoch: 32/500\tLoss: 2.100994\n",
            "Epoch: 33/500\tLoss: 2.115837\n",
            "Epoch: 34/500\tLoss: 2.102108\n",
            "Epoch: 35/500\tLoss: 2.077385\n",
            "Epoch: 36/500\tLoss: 2.072026\n",
            "Epoch: 37/500\tLoss: 2.091205\n",
            "Epoch: 38/500\tLoss: 2.088489\n",
            "Epoch: 39/500\tLoss: 2.096267\n",
            "****\n",
            "time traveller the this the this the this the this the this the t\n",
            "****\n",
            "Epoch: 40/500\tLoss: 2.043519\n",
            "Epoch: 41/500\tLoss: 2.069338\n",
            "Epoch: 42/500\tLoss: 2.045712\n",
            "Epoch: 43/500\tLoss: 2.062033\n",
            "Epoch: 44/500\tLoss: 2.021324\n",
            "Epoch: 45/500\tLoss: 2.025951\n",
            "Epoch: 46/500\tLoss: 2.050863\n",
            "Epoch: 47/500\tLoss: 2.009263\n",
            "Epoch: 48/500\tLoss: 2.015307\n",
            "Epoch: 49/500\tLoss: 1.994631\n",
            "****\n",
            "time traveller then this this somentinnt of thace the thing thang\n",
            "****\n",
            "Epoch: 50/500\tLoss: 2.001702\n",
            "Epoch: 51/500\tLoss: 2.011585\n",
            "Epoch: 52/500\tLoss: 2.018825\n",
            "Epoch: 53/500\tLoss: 1.985536\n",
            "Epoch: 54/500\tLoss: 1.954530\n",
            "Epoch: 55/500\tLoss: 1.978755\n",
            "Epoch: 56/500\tLoss: 2.005591\n",
            "Epoch: 57/500\tLoss: 1.968133\n",
            "Epoch: 58/500\tLoss: 1.933428\n",
            "Epoch: 59/500\tLoss: 1.926461\n",
            "****\n",
            "time traveller and the the that and the the thas ing thave and th\n",
            "****\n",
            "Epoch: 60/500\tLoss: 1.917990\n",
            "Epoch: 61/500\tLoss: 1.926017\n",
            "Epoch: 62/500\tLoss: 1.923930\n",
            "Epoch: 63/500\tLoss: 1.921070\n",
            "Epoch: 64/500\tLoss: 1.884350\n",
            "Epoch: 65/500\tLoss: 1.866637\n",
            "Epoch: 66/500\tLoss: 1.925322\n",
            "Epoch: 67/500\tLoss: 1.849953\n",
            "Epoch: 68/500\tLoss: 1.855558\n",
            "Epoch: 69/500\tLoss: 1.821384\n",
            "****\n",
            "time traveller and that in that ghere and the that bred thithed t\n",
            "****\n",
            "Epoch: 70/500\tLoss: 1.848115\n",
            "Epoch: 71/500\tLoss: 1.797851\n",
            "Epoch: 72/500\tLoss: 1.814858\n",
            "Epoch: 73/500\tLoss: 1.771477\n",
            "Epoch: 74/500\tLoss: 1.790840\n",
            "Epoch: 75/500\tLoss: 1.778700\n",
            "Epoch: 76/500\tLoss: 1.756159\n",
            "Epoch: 77/500\tLoss: 1.740765\n",
            "Epoch: 78/500\tLoss: 1.740661\n",
            "Epoch: 79/500\tLoss: 1.701087\n",
            "****\n",
            "time traveller and thave at ala that is allyot ious in the thay h\n",
            "****\n",
            "Epoch: 80/500\tLoss: 1.682201\n",
            "Epoch: 81/500\tLoss: 1.660121\n",
            "Epoch: 82/500\tLoss: 1.658470\n",
            "Epoch: 83/500\tLoss: 1.622862\n",
            "Epoch: 84/500\tLoss: 1.627745\n",
            "Epoch: 85/500\tLoss: 1.607790\n",
            "Epoch: 86/500\tLoss: 1.590270\n",
            "Epoch: 87/500\tLoss: 1.546012\n",
            "Epoch: 88/500\tLoss: 1.530181\n",
            "Epoch: 89/500\tLoss: 1.522773\n",
            "****\n",
            "time traveller cores in this in th thicelo is all ston thicendis \n",
            "****\n",
            "Epoch: 90/500\tLoss: 1.484190\n",
            "Epoch: 91/500\tLoss: 1.454816\n",
            "Epoch: 92/500\tLoss: 1.472485\n",
            "Epoch: 93/500\tLoss: 1.441786\n",
            "Epoch: 94/500\tLoss: 1.423461\n",
            "Epoch: 95/500\tLoss: 1.393110\n",
            "Epoch: 96/500\tLoss: 1.373966\n",
            "Epoch: 97/500\tLoss: 1.356899\n",
            "Epoch: 98/500\tLoss: 1.347800\n",
            "Epoch: 99/500\tLoss: 1.329111\n",
            "****\n",
            "time traveller thicurdine sin this is thethis thit nower ans so d\n",
            "****\n",
            "Epoch: 100/500\tLoss: 1.341441\n",
            "Epoch: 101/500\tLoss: 1.320194\n",
            "Epoch: 102/500\tLoss: 1.300570\n",
            "Epoch: 103/500\tLoss: 1.291944\n",
            "Epoch: 104/500\tLoss: 1.283991\n",
            "Epoch: 105/500\tLoss: 1.250661\n",
            "Epoch: 106/500\tLoss: 1.244411\n",
            "Epoch: 107/500\tLoss: 1.214195\n",
            "Epoch: 108/500\tLoss: 1.182735\n",
            "Epoch: 109/500\tLoss: 1.174092\n",
            "****\n",
            "time traveller pareef le than whis fore whing a foct this filbs a\n",
            "****\n",
            "Epoch: 110/500\tLoss: 1.212741\n",
            "Epoch: 111/500\tLoss: 1.169027\n",
            "Epoch: 112/500\tLoss: 1.134552\n",
            "Epoch: 113/500\tLoss: 1.106506\n",
            "Epoch: 114/500\tLoss: 1.105872\n",
            "Epoch: 115/500\tLoss: 1.125727\n",
            "Epoch: 116/500\tLoss: 1.108818\n",
            "Epoch: 117/500\tLoss: 1.066655\n",
            "Epoch: 118/500\tLoss: 1.036774\n",
            "Epoch: 119/500\tLoss: 1.070887\n",
            "****\n",
            "time traveller butwe this the trime sime sion ar d and and and ch\n",
            "****\n",
            "Epoch: 120/500\tLoss: 1.046483\n",
            "Epoch: 121/500\tLoss: 1.039714\n",
            "Epoch: 122/500\tLoss: 1.024186\n",
            "Epoch: 123/500\tLoss: 1.039326\n",
            "Epoch: 124/500\tLoss: 1.001650\n",
            "Epoch: 125/500\tLoss: 0.978368\n",
            "Epoch: 126/500\tLoss: 0.965561\n",
            "Epoch: 127/500\tLoss: 0.974913\n",
            "Epoch: 128/500\tLoss: 0.946131\n",
            "Epoch: 129/500\tLoss: 0.930503\n",
            "****\n",
            "time traveller for and are the ftreat dithiss ican sire aread i b\n",
            "****\n",
            "Epoch: 130/500\tLoss: 0.967488\n",
            "Epoch: 131/500\tLoss: 0.942435\n",
            "Epoch: 132/500\tLoss: 0.914116\n",
            "Epoch: 133/500\tLoss: 0.912613\n",
            "Epoch: 134/500\tLoss: 0.937588\n",
            "Epoch: 135/500\tLoss: 0.888777\n",
            "Epoch: 136/500\tLoss: 0.930222\n",
            "Epoch: 137/500\tLoss: 0.864719\n",
            "Epoch: 138/500\tLoss: 0.876885\n",
            "Epoch: 139/500\tLoss: 0.859871\n",
            "****\n",
            "time traveller psocu and and shatly in the dimentiony urf the thi\n",
            "****\n",
            "Epoch: 140/500\tLoss: 0.842290\n",
            "Epoch: 141/500\tLoss: 0.835922\n",
            "Epoch: 142/500\tLoss: 0.823734\n",
            "Epoch: 143/500\tLoss: 0.811304\n",
            "Epoch: 144/500\tLoss: 0.811440\n",
            "Epoch: 145/500\tLoss: 0.830502\n",
            "Epoch: 146/500\tLoss: 0.764522\n",
            "Epoch: 147/500\tLoss: 0.761284\n",
            "Epoch: 148/500\tLoss: 0.778918\n",
            "Epoch: 149/500\tLoss: 0.757670\n",
            "****\n",
            "time traveller hat nithe time hravellerthe thin wist eftren his d\n",
            "****\n",
            "Epoch: 150/500\tLoss: 0.786298\n",
            "Epoch: 151/500\tLoss: 0.789729\n",
            "Epoch: 152/500\tLoss: 0.741192\n",
            "Epoch: 153/500\tLoss: 0.751981\n",
            "Epoch: 154/500\tLoss: 0.715147\n",
            "Epoch: 155/500\tLoss: 0.714566\n",
            "Epoch: 156/500\tLoss: 0.720292\n",
            "Epoch: 157/500\tLoss: 0.720230\n",
            "Epoch: 158/500\tLoss: 0.713840\n",
            "Epoch: 159/500\tLoss: 0.703153\n",
            "****\n",
            "time traveller hew of song tientitn of the toree sablea dicension\n",
            "****\n",
            "Epoch: 160/500\tLoss: 0.689705\n",
            "Epoch: 161/500\tLoss: 0.707216\n",
            "Epoch: 162/500\tLoss: 0.717872\n",
            "Epoch: 163/500\tLoss: 0.665737\n",
            "Epoch: 164/500\tLoss: 0.683031\n",
            "Epoch: 165/500\tLoss: 0.668914\n",
            "Epoch: 166/500\tLoss: 0.697176\n",
            "Epoch: 167/500\tLoss: 0.646751\n",
            "Epoch: 168/500\tLoss: 0.654901\n",
            "Epoch: 169/500\tLoss: 0.647884\n",
            "****\n",
            "time traveller hrod ane o mectwar we s ou the time time dimension\n",
            "****\n",
            "Epoch: 170/500\tLoss: 0.673445\n",
            "Epoch: 171/500\tLoss: 0.652145\n",
            "Epoch: 172/500\tLoss: 0.623939\n",
            "Epoch: 173/500\tLoss: 0.618419\n",
            "Epoch: 174/500\tLoss: 0.619804\n",
            "Epoch: 175/500\tLoss: 0.642826\n",
            "Epoch: 176/500\tLoss: 0.642636\n",
            "Epoch: 177/500\tLoss: 0.637271\n",
            "Epoch: 178/500\tLoss: 0.599590\n",
            "Epoch: 179/500\tLoss: 0.598868\n",
            "****\n",
            "time traveller he duther here the this ball onith was of the from\n",
            "****\n",
            "Epoch: 180/500\tLoss: 0.601122\n",
            "Epoch: 181/500\tLoss: 0.629221\n",
            "Epoch: 182/500\tLoss: 0.586093\n",
            "Epoch: 183/500\tLoss: 0.594151\n",
            "Epoch: 184/500\tLoss: 0.562536\n",
            "Epoch: 185/500\tLoss: 0.569393\n",
            "Epoch: 186/500\tLoss: 0.578928\n",
            "Epoch: 187/500\tLoss: 0.584320\n",
            "Epoch: 188/500\tLoss: 0.558515\n",
            "Epoch: 189/500\tLoss: 0.553783\n",
            "****\n",
            "time traveller froc efsethine in to vera llard ant spoo mere sabe\n",
            "****\n",
            "Epoch: 190/500\tLoss: 0.544960\n",
            "Epoch: 191/500\tLoss: 0.533809\n",
            "Epoch: 192/500\tLoss: 0.540611\n",
            "Epoch: 193/500\tLoss: 0.537089\n",
            "Epoch: 194/500\tLoss: 0.529315\n",
            "Epoch: 195/500\tLoss: 0.530374\n",
            "Epoch: 196/500\tLoss: 0.533106\n",
            "Epoch: 197/500\tLoss: 0.528067\n",
            "Epoch: 198/500\tLoss: 0.536927\n",
            "Epoch: 199/500\tLoss: 0.549690\n",
            "****\n",
            "time traveller hes aid aisuis and wing ag the thime sian wis was \n",
            "****\n",
            "Epoch: 200/500\tLoss: 0.550094\n",
            "Epoch: 201/500\tLoss: 0.517463\n",
            "Epoch: 202/500\tLoss: 0.511591\n",
            "Epoch: 203/500\tLoss: 0.516647\n",
            "Epoch: 204/500\tLoss: 0.541986\n",
            "Epoch: 205/500\tLoss: 0.514686\n",
            "Epoch: 206/500\tLoss: 0.522588\n",
            "Epoch: 207/500\tLoss: 0.532909\n",
            "Epoch: 208/500\tLoss: 0.533786\n",
            "Epoch: 209/500\tLoss: 0.491718\n",
            "****\n",
            "time traveller proceeded anyreal hodear shallans mone ableas said\n",
            "****\n",
            "Epoch: 210/500\tLoss: 0.549052\n",
            "Epoch: 211/500\tLoss: 0.508731\n",
            "Epoch: 212/500\tLoss: 0.484961\n",
            "Epoch: 213/500\tLoss: 0.478025\n",
            "Epoch: 214/500\tLoss: 0.477023\n",
            "Epoch: 215/500\tLoss: 0.473847\n",
            "Epoch: 216/500\tLoss: 0.473900\n",
            "Epoch: 217/500\tLoss: 0.465329\n",
            "Epoch: 218/500\tLoss: 0.453847\n",
            "Epoch: 219/500\tLoss: 0.489457\n",
            "****\n",
            "time traveller came back andfilby s ane iume ary allbugand yia ce\n",
            "****\n",
            "Epoch: 220/500\tLoss: 0.466108\n",
            "Epoch: 221/500\tLoss: 0.475620\n",
            "Epoch: 222/500\tLoss: 0.489413\n",
            "Epoch: 223/500\tLoss: 0.442496\n",
            "Epoch: 224/500\tLoss: 0.453846\n",
            "Epoch: 225/500\tLoss: 0.489283\n",
            "Epoch: 226/500\tLoss: 0.468776\n",
            "Epoch: 227/500\tLoss: 0.464447\n",
            "Epoch: 228/500\tLoss: 0.457497\n",
            "Epoch: 229/500\tLoss: 0.503045\n",
            "****\n",
            "time traveller for so it wislong to vey the peotle tho time thane\n",
            "****\n",
            "Epoch: 230/500\tLoss: 0.484474\n",
            "Epoch: 231/500\tLoss: 0.424911\n",
            "Epoch: 232/500\tLoss: 0.438107\n",
            "Epoch: 233/500\tLoss: 0.462172\n",
            "Epoch: 234/500\tLoss: 0.469158\n",
            "Epoch: 235/500\tLoss: 0.435936\n",
            "Epoch: 236/500\tLoss: 0.435133\n",
            "Epoch: 237/500\tLoss: 0.411910\n",
            "Epoch: 238/500\tLoss: 0.447316\n",
            "Epoch: 239/500\tLoss: 0.435655\n",
            "****\n",
            "time traveller for so it wiscoubur teat an whing be bettrenous in\n",
            "****\n",
            "Epoch: 240/500\tLoss: 0.486407\n",
            "Epoch: 241/500\tLoss: 0.430309\n",
            "Epoch: 242/500\tLoss: 0.450113\n",
            "Epoch: 243/500\tLoss: 0.416833\n",
            "Epoch: 244/500\tLoss: 0.423966\n",
            "Epoch: 245/500\tLoss: 0.398419\n",
            "Epoch: 246/500\tLoss: 0.428546\n",
            "Epoch: 247/500\tLoss: 0.407708\n",
            "Epoch: 248/500\tLoss: 0.424179\n",
            "Epoch: 249/500\tLoss: 0.464761\n",
            "****\n",
            "time traveller herd in time brac llorepsly certhet dof have ix co\n",
            "****\n",
            "Epoch: 250/500\tLoss: 0.393278\n",
            "Epoch: 251/500\tLoss: 0.419229\n",
            "Epoch: 252/500\tLoss: 0.450928\n",
            "Epoch: 253/500\tLoss: 0.440414\n",
            "Epoch: 254/500\tLoss: 0.424933\n",
            "Epoch: 255/500\tLoss: 0.428145\n",
            "Epoch: 256/500\tLoss: 0.383186\n",
            "Epoch: 257/500\tLoss: 0.405702\n",
            "Epoch: 258/500\tLoss: 0.400995\n",
            "Epoch: 259/500\tLoss: 0.391387\n",
            "****\n",
            "time traveller proce ted his sagllast has ingtiny hall anot thick\n",
            "****\n",
            "Epoch: 260/500\tLoss: 0.420070\n",
            "Epoch: 261/500\tLoss: 0.384089\n",
            "Epoch: 262/500\tLoss: 0.387818\n",
            "Epoch: 263/500\tLoss: 0.397531\n",
            "Epoch: 264/500\tLoss: 0.382154\n",
            "Epoch: 265/500\tLoss: 0.373900\n",
            "Epoch: 266/500\tLoss: 0.434476\n",
            "Epoch: 267/500\tLoss: 0.388509\n",
            "Epoch: 268/500\tLoss: 0.462941\n",
            "Epoch: 269/500\tLoss: 0.410051\n",
            "****\n",
            "time traveller after the paller douring thal seraccit toly wall g\n",
            "****\n",
            "Epoch: 270/500\tLoss: 0.406098\n",
            "Epoch: 271/500\tLoss: 0.368002\n",
            "Epoch: 272/500\tLoss: 0.392939\n",
            "Epoch: 273/500\tLoss: 0.413966\n",
            "Epoch: 274/500\tLoss: 0.389876\n",
            "Epoch: 275/500\tLoss: 0.405842\n",
            "Epoch: 276/500\tLoss: 0.405984\n",
            "Epoch: 277/500\tLoss: 0.391217\n",
            "Epoch: 278/500\tLoss: 0.403899\n",
            "Epoch: 279/500\tLoss: 0.371012\n",
            "****\n",
            "time traveller after the pauserequired for the proper assicilatio\n",
            "****\n",
            "Epoch: 280/500\tLoss: 0.384435\n",
            "Epoch: 281/500\tLoss: 0.407045\n",
            "Epoch: 282/500\tLoss: 0.420602\n",
            "Epoch: 283/500\tLoss: 0.376574\n",
            "Epoch: 284/500\tLoss: 0.374392\n",
            "Epoch: 285/500\tLoss: 0.389024\n",
            "Epoch: 286/500\tLoss: 0.369470\n",
            "Epoch: 287/500\tLoss: 0.389922\n",
            "Epoch: 288/500\tLoss: 0.395534\n",
            "Epoch: 289/500\tLoss: 0.364681\n",
            "****\n",
            "time traveller came back anstimentime pastreet lard trect as he w\n",
            "****\n",
            "Epoch: 290/500\tLoss: 0.377131\n",
            "Epoch: 291/500\tLoss: 0.389626\n",
            "Epoch: 292/500\tLoss: 0.374293\n",
            "Epoch: 293/500\tLoss: 0.394099\n",
            "Epoch: 294/500\tLoss: 0.394526\n",
            "Epoch: 295/500\tLoss: 0.393945\n",
            "Epoch: 296/500\tLoss: 0.364931\n",
            "Epoch: 297/500\tLoss: 0.357050\n",
            "Epoch: 298/500\tLoss: 0.350204\n",
            "Epoch: 299/500\tLoss: 0.377999\n",
            "****\n",
            "time traveller ard and scand his frouthes that somed bad mo kighe\n",
            "****\n",
            "Epoch: 300/500\tLoss: 0.381795\n",
            "Epoch: 301/500\tLoss: 0.391209\n",
            "Epoch: 302/500\tLoss: 0.362246\n",
            "Epoch: 303/500\tLoss: 0.368629\n",
            "Epoch: 304/500\tLoss: 0.363030\n",
            "Epoch: 305/500\tLoss: 0.419544\n",
            "Epoch: 306/500\tLoss: 0.372801\n",
            "Epoch: 307/500\tLoss: 0.342075\n",
            "Epoch: 308/500\tLoss: 0.374996\n",
            "Epoch: 309/500\tLoss: 0.378160\n",
            "****\n",
            "time traveller trmereas high wast le parnet y is is the frou d an\n",
            "****\n",
            "Epoch: 310/500\tLoss: 0.348654\n",
            "Epoch: 311/500\tLoss: 0.343227\n",
            "Epoch: 312/500\tLoss: 0.382283\n",
            "Epoch: 313/500\tLoss: 0.353367\n",
            "Epoch: 314/500\tLoss: 0.320038\n",
            "Epoch: 315/500\tLoss: 0.356894\n",
            "Epoch: 316/500\tLoss: 0.392113\n",
            "Epoch: 317/500\tLoss: 0.349892\n",
            "Epoch: 318/500\tLoss: 0.353414\n",
            "Epoch: 319/500\tLoss: 0.365091\n",
            "****\n",
            "time traveller ano unar i bas aravell ploce abde four madinblans \n",
            "****\n",
            "Epoch: 320/500\tLoss: 0.413349\n",
            "Epoch: 321/500\tLoss: 0.404698\n",
            "Epoch: 322/500\tLoss: 0.366515\n",
            "Epoch: 323/500\tLoss: 0.324442\n",
            "Epoch: 324/500\tLoss: 0.380180\n",
            "Epoch: 325/500\tLoss: 0.348579\n",
            "Epoch: 326/500\tLoss: 0.343800\n",
            "Epoch: 327/500\tLoss: 0.354344\n",
            "Epoch: 328/500\tLoss: 0.357171\n",
            "Epoch: 329/500\tLoss: 0.356277\n",
            "****\n",
            "time traveller held in ti bick sperthan monet aly andmuwnyco io j\n",
            "****\n",
            "Epoch: 330/500\tLoss: 0.367428\n",
            "Epoch: 331/500\tLoss: 0.348111\n",
            "Epoch: 332/500\tLoss: 0.340145\n",
            "Epoch: 333/500\tLoss: 0.364447\n",
            "Epoch: 334/500\tLoss: 0.334145\n",
            "Epoch: 335/500\tLoss: 0.372852\n",
            "Epoch: 336/500\tLoss: 0.325452\n",
            "Epoch: 337/500\tLoss: 0.349748\n",
            "Epoch: 338/500\tLoss: 0.338576\n",
            "Epoch: 339/500\tLoss: 0.341359\n",
            "****\n",
            "time traveller held insearof shechear and har abuug taised for ch\n",
            "****\n",
            "Epoch: 340/500\tLoss: 0.356935\n",
            "Epoch: 341/500\tLoss: 0.352741\n",
            "Epoch: 342/500\tLoss: 0.311325\n",
            "Epoch: 343/500\tLoss: 0.362157\n",
            "Epoch: 344/500\tLoss: 0.354566\n",
            "Epoch: 345/500\tLoss: 0.357581\n",
            "Epoch: 346/500\tLoss: 0.354073\n",
            "Epoch: 347/500\tLoss: 0.337199\n",
            "Epoch: 348/500\tLoss: 0.356915\n",
            "Epoch: 349/500\tLoss: 0.335729\n",
            "****\n",
            "time traveller held in his hand was a glitteringmetallic framewor\n",
            "****\n",
            "Epoch: 350/500\tLoss: 0.384598\n",
            "Epoch: 351/500\tLoss: 0.364486\n",
            "Epoch: 352/500\tLoss: 0.364689\n",
            "Epoch: 353/500\tLoss: 0.357634\n",
            "Epoch: 354/500\tLoss: 0.349700\n",
            "Epoch: 355/500\tLoss: 0.393661\n",
            "Epoch: 356/500\tLoss: 0.366953\n",
            "Epoch: 357/500\tLoss: 0.361172\n",
            "Epoch: 358/500\tLoss: 0.334567\n",
            "Epoch: 359/500\tLoss: 0.352415\n",
            "****\n",
            "time traveller after the psuchely have at ofive saknedthe time tr\n",
            "****\n",
            "Epoch: 360/500\tLoss: 0.395725\n",
            "Epoch: 361/500\tLoss: 0.323300\n",
            "Epoch: 362/500\tLoss: 0.347810\n",
            "Epoch: 363/500\tLoss: 0.385395\n",
            "Epoch: 364/500\tLoss: 0.344702\n",
            "Epoch: 365/500\tLoss: 0.332783\n",
            "Epoch: 366/500\tLoss: 0.343802\n",
            "Epoch: 367/500\tLoss: 0.375600\n",
            "Epoch: 368/500\tLoss: 0.341184\n",
            "Epoch: 369/500\tLoss: 0.322609\n",
            "****\n",
            "time traveller held in his hand was a glitteriol spack and are y \n",
            "****\n",
            "Epoch: 370/500\tLoss: 0.345335\n",
            "Epoch: 371/500\tLoss: 0.330417\n",
            "Epoch: 372/500\tLoss: 0.350561\n",
            "Epoch: 373/500\tLoss: 0.336549\n",
            "Epoch: 374/500\tLoss: 0.342284\n",
            "Epoch: 375/500\tLoss: 0.309770\n",
            "Epoch: 376/500\tLoss: 0.353227\n",
            "Epoch: 377/500\tLoss: 0.334916\n",
            "Epoch: 378/500\tLoss: 0.389185\n",
            "Epoch: 379/500\tLoss: 0.329647\n",
            "****\n",
            "time traveller but now you begin to seethe object or man and his \n",
            "****\n",
            "Epoch: 380/500\tLoss: 0.313023\n",
            "Epoch: 381/500\tLoss: 0.356997\n",
            "Epoch: 382/500\tLoss: 0.324862\n",
            "Epoch: 383/500\tLoss: 0.315482\n",
            "Epoch: 384/500\tLoss: 0.360834\n",
            "Epoch: 385/500\tLoss: 0.335183\n",
            "Epoch: 386/500\tLoss: 0.321206\n",
            "Epoch: 387/500\tLoss: 0.330918\n",
            "Epoch: 388/500\tLoss: 0.340846\n",
            "Epoch: 389/500\tLoss: 0.369324\n",
            "****\n",
            "time traveller for anding the time traveller but now you begin to\n",
            "****\n",
            "Epoch: 390/500\tLoss: 0.336401\n",
            "Epoch: 391/500\tLoss: 0.319729\n",
            "Epoch: 392/500\tLoss: 0.339235\n",
            "Epoch: 393/500\tLoss: 0.335304\n",
            "Epoch: 394/500\tLoss: 0.332622\n",
            "Epoch: 395/500\tLoss: 0.340720\n",
            "Epoch: 396/500\tLoss: 0.311466\n",
            "Epoch: 397/500\tLoss: 0.335292\n",
            "Epoch: 398/500\tLoss: 0.371343\n",
            "Epoch: 399/500\tLoss: 0.356477\n",
            "****\n",
            "time traveller but now you begin to seethe object of my the es th\n",
            "****\n",
            "Epoch: 400/500\tLoss: 0.356971\n",
            "Epoch: 401/500\tLoss: 0.332487\n",
            "Epoch: 402/500\tLoss: 0.351818\n",
            "Epoch: 403/500\tLoss: 0.346838\n",
            "Epoch: 404/500\tLoss: 0.337180\n",
            "Epoch: 405/500\tLoss: 0.378143\n",
            "Epoch: 406/500\tLoss: 0.333062\n",
            "Epoch: 407/500\tLoss: 0.299523\n",
            "Epoch: 408/500\tLoss: 0.364924\n",
            "Epoch: 409/500\tLoss: 0.369669\n",
            "****\n",
            "time traveller proceeded anyrero so woul the ouli have thing bati\n",
            "****\n",
            "Epoch: 410/500\tLoss: 0.327940\n",
            "Epoch: 411/500\tLoss: 0.327384\n",
            "Epoch: 412/500\tLoss: 0.327361\n",
            "Epoch: 413/500\tLoss: 0.358823\n",
            "Epoch: 414/500\tLoss: 0.332728\n",
            "Epoch: 415/500\tLoss: 0.310170\n",
            "Epoch: 416/500\tLoss: 0.336736\n",
            "Epoch: 417/500\tLoss: 0.310704\n",
            "Epoch: 418/500\tLoss: 0.360946\n",
            "Epoch: 419/500\tLoss: 0.368111\n",
            "****\n",
            "time traveller for so it will be convenient to speak of himwas ex\n",
            "****\n",
            "Epoch: 420/500\tLoss: 0.324814\n",
            "Epoch: 421/500\tLoss: 0.341292\n",
            "Epoch: 422/500\tLoss: 0.364243\n",
            "Epoch: 423/500\tLoss: 0.344984\n",
            "Epoch: 424/500\tLoss: 0.336391\n",
            "Epoch: 425/500\tLoss: 0.312857\n",
            "Epoch: 426/500\tLoss: 0.324197\n",
            "Epoch: 427/500\tLoss: 0.334878\n",
            "Epoch: 428/500\tLoss: 0.306952\n",
            "Epoch: 429/500\tLoss: 0.314048\n",
            "****\n",
            "time traveller for so it will beisction if f and anottinn sime an\n",
            "****\n",
            "Epoch: 430/500\tLoss: 0.328591\n",
            "Epoch: 431/500\tLoss: 0.322890\n",
            "Epoch: 432/500\tLoss: 0.369503\n",
            "Epoch: 433/500\tLoss: 0.332403\n",
            "Epoch: 434/500\tLoss: 0.343508\n",
            "Epoch: 435/500\tLoss: 0.334460\n",
            "Epoch: 436/500\tLoss: 0.323093\n",
            "Epoch: 437/500\tLoss: 0.312019\n",
            "Epoch: 438/500\tLoss: 0.328766\n",
            "Epoch: 439/500\tLoss: 0.307539\n",
            "****\n",
            "time traveller after the pall have thing wal tarontwhe haslong mo\n",
            "****\n",
            "Epoch: 440/500\tLoss: 0.329960\n",
            "Epoch: 441/500\tLoss: 0.323190\n",
            "Epoch: 442/500\tLoss: 0.314505\n",
            "Epoch: 443/500\tLoss: 0.305625\n",
            "Epoch: 444/500\tLoss: 0.343620\n",
            "Epoch: 445/500\tLoss: 0.316375\n",
            "Epoch: 446/500\tLoss: 0.319136\n",
            "Epoch: 447/500\tLoss: 0.326358\n",
            "Epoch: 448/500\tLoss: 0.372279\n",
            "Epoch: 449/500\tLoss: 0.352665\n",
            "****\n",
            "time traveller proceeded anyreal body must have extension in four\n",
            "****\n",
            "Epoch: 450/500\tLoss: 0.354615\n",
            "Epoch: 451/500\tLoss: 0.330947\n",
            "Epoch: 452/500\tLoss: 0.354865\n",
            "Epoch: 453/500\tLoss: 0.311845\n",
            "Epoch: 454/500\tLoss: 0.310074\n",
            "Epoch: 455/500\tLoss: 0.293864\n",
            "Epoch: 456/500\tLoss: 0.346323\n",
            "Epoch: 457/500\tLoss: 0.281156\n",
            "Epoch: 458/500\tLoss: 0.317268\n",
            "Epoch: 459/500\tLoss: 0.331491\n",
            "****\n",
            "time traveller held in his hand was a glittert is tely in instris\n",
            "****\n",
            "Epoch: 460/500\tLoss: 0.299506\n",
            "Epoch: 461/500\tLoss: 0.332777\n",
            "Epoch: 462/500\tLoss: 0.372679\n",
            "Epoch: 463/500\tLoss: 0.310456\n",
            "Epoch: 464/500\tLoss: 0.316194\n",
            "Epoch: 465/500\tLoss: 0.293912\n",
            "Epoch: 466/500\tLoss: 0.318602\n",
            "Epoch: 467/500\tLoss: 0.336830\n",
            "Epoch: 468/500\tLoss: 0.312967\n",
            "Epoch: 469/500\tLoss: 0.321884\n",
            "****\n",
            "time traveller but now you begin to seethe object of my investiga\n",
            "****\n",
            "Epoch: 470/500\tLoss: 0.327252\n",
            "Epoch: 471/500\tLoss: 0.306743\n",
            "Epoch: 472/500\tLoss: 0.319618\n",
            "Epoch: 473/500\tLoss: 0.349574\n",
            "Epoch: 474/500\tLoss: 0.318409\n",
            "Epoch: 475/500\tLoss: 0.321563\n",
            "Epoch: 476/500\tLoss: 0.292491\n",
            "Epoch: 477/500\tLoss: 0.313858\n",
            "Epoch: 478/500\tLoss: 0.323020\n",
            "Epoch: 479/500\tLoss: 0.305253\n",
            "****\n",
            "time traveller for so it will be convenient tor shallhinelonst of\n",
            "****\n",
            "Epoch: 480/500\tLoss: 0.302607\n",
            "Epoch: 481/500\tLoss: 0.331660\n",
            "Epoch: 482/500\tLoss: 0.326913\n",
            "Epoch: 483/500\tLoss: 0.296481\n",
            "Epoch: 484/500\tLoss: 0.375518\n",
            "Epoch: 485/500\tLoss: 0.337064\n",
            "Epoch: 486/500\tLoss: 0.323202\n",
            "Epoch: 487/500\tLoss: 0.319926\n",
            "Epoch: 488/500\tLoss: 0.302714\n",
            "Epoch: 489/500\tLoss: 0.343711\n",
            "****\n",
            "time traveller proceeded an reallyot int an ithree mimensional so\n",
            "****\n",
            "Epoch: 490/500\tLoss: 0.312855\n",
            "Epoch: 491/500\tLoss: 0.314775\n",
            "Epoch: 492/500\tLoss: 0.306096\n",
            "Epoch: 493/500\tLoss: 0.300992\n",
            "Epoch: 494/500\tLoss: 0.359253\n",
            "Epoch: 495/500\tLoss: 0.313069\n",
            "Epoch: 496/500\tLoss: 0.289469\n",
            "Epoch: 497/500\tLoss: 0.304204\n",
            "Epoch: 498/500\tLoss: 0.290922\n",
            "Epoch: 499/500\tLoss: 0.325911\n",
            "****\n",
            "time traveller proceeded anyreal body must have extension in for \n",
            "****\n",
            "Epoch: 500/500\tLoss: 0.301415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yEwrzUyebodv",
        "outputId": "f4d7c2c9-7eac-4196-d21f-e9335b1350f1"
      },
      "source": [
        "predict('hi lana wonder how ', num_preds=120, net=net, vocab=vocab, device=device)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hi lana wonder how stay ie a fised in thithine breall inot butingon is and why cannotwe move in time as we move about in timethat is the ge'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RF50P_CeJBr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}