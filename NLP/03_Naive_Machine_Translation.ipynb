{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Naive_Machine_Translation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MW_KerHnQBm"
      },
      "source": [
        "# Program that translates English to French"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAQNN5s6mX5y"
      },
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "import time\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9fvtFVVnCH-",
        "outputId": "2f38e086-916a-4036-f84b-336f48ed13ee"
      },
      "source": [
        "# download google news word2vec\n",
        "\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-26 11:02:50--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.147.118\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.147.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‚ÄòGoogleNews-vectors-negative300.bin.gz‚Äô\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  41.5MB/s    in 40s     \n",
            "\n",
            "2021-06-26 11:03:30 (39.7 MB/s) - ‚ÄòGoogleNews-vectors-negative300.bin.gz‚Äô saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNgOJoLkoO7a"
      },
      "source": [
        "# unzip the downloaded file\n",
        "!gzip -d GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxIuuH80odjP",
        "outputId": "c74286cd-4ec4-4355-9b08-b2912ed8b7c9"
      },
      "source": [
        "# french embeddings\n",
        "!curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  599M  100  599M    0     0  50.8M      0  0:00:11  0:00:11 --:--:-- 55.9M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuBtqG4Zo3h_"
      },
      "source": [
        "en_embeddings = KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "fr_embeddings = KeyedVectors.load_word2vec_format('/content/wiki.multi.fr.vec')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRXQjjTsqfbx"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJAwlozEpQw8"
      },
      "source": [
        "def get_dict(filename):\n",
        "  '''\n",
        "    This function returns the english to french dictionary given a file where each column corresponds to a word.\n",
        "  '''\n",
        "  my_file = pd.read_csv(filename, sep=' ')\n",
        "  etof = {}\n",
        "  for i in range(len(my_file)):\n",
        "    en = my_file.iloc[i,0]\n",
        "    fr = my_file.iloc[i,1]\n",
        "    etof[en] = fr\n",
        "  return etof\n",
        "\n",
        "def cosine_similarity(A,B):\n",
        "  \"\"\"\n",
        "  Input: \n",
        "    A: a numpy array corresponds to a word vector\n",
        "    B: a numpy array corresponds to a word vector\n",
        "  Output: \n",
        "    cos: numerical number representing the cosine similarity between A and B\n",
        "  \"\"\"\n",
        "  dot = np.dot(A, B)\n",
        "  norma = np.linalg.norm(A)\n",
        "  normb = np.linalg.norm(B)\n",
        "  cos = dot / (norma * normb)\n",
        "  return cos"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BomE--yr62t"
      },
      "source": [
        "## Loading the english to french dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqZaggpFrSOK",
        "outputId": "c51d366c-fe3b-4b9c-98a8-1f4d52b450dd"
      },
      "source": [
        "en_fr_train = get_dict('/content/en-fr.train.txt')\n",
        "print(f'The length of the english to french training dictionary is {len(en_fr_train)}')\n",
        "en_fr_test = get_dict('/content/en-fr.test.txt')\n",
        "print(f'The length of the english to french test dictionary is {len(en_fr_test)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the english to french training dictionary is 5000\n",
            "The length of the english to french test dictionary is 1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtHhkfqwELi"
      },
      "source": [
        "english_set = set(en_embeddings.vocab)\n",
        "french_set = set(fr_embeddings.vocab)\n",
        "en_embeddings_subset = {}\n",
        "fr_embeddings_subset = {}\n",
        "french_words = set(en_fr_train.values())\n",
        "\n",
        "for en_word in en_fr_train.keys():\n",
        "  fr_word = en_fr_train[en_word]\n",
        "  if fr_word in french_set and en_word in english_set:\n",
        "    en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "    fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "for en_word in en_fr_test.keys():\n",
        "  fr_word = en_fr_test[en_word]\n",
        "  if fr_word in french_set and en_word in english_set:\n",
        "    en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "    fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-ysCTrxoH7",
        "outputId": "354ffc2d-da71-4e6b-e6c9-d512fc8b506e"
      },
      "source": [
        "# embedding dimensions\n",
        "print(en_embeddings_subset['the'].shape[0])\n",
        "print(fr_embeddings_subset['la'].shape[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgmhTgAoy2D3"
      },
      "source": [
        "## Building matrices X and Y\n",
        "\n",
        "X: mxn matrix, where m = number of rows corresponding to m **english** words; n = embedding dimensions\n",
        "\n",
        "Y: mxn matrix, where m = number of rows corresponding to m **french** words; n = embedding dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r7KZTCAx_d_"
      },
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    en_fr: English to french dictionary\n",
        "    french_vecs: French words to their corresponding word embeddings\n",
        "    english_vecs: English words to their corresponding word embeddings\n",
        "  Output: \n",
        "    X: a matrix where the columns are the English embeddings\n",
        "    Y: a matrix where the columns are the French embeddings\n",
        "    R: the projection matrix that minimizes the F norm ||XR - Y||^2\n",
        "  \"\"\"\n",
        "  \n",
        "  X_l = list()\n",
        "  Y_l = list()\n",
        "\n",
        "  english_set = set(english_vecs.keys())\n",
        "  french_set = set(french_vecs.keys())\n",
        "\n",
        "  french_words = set(en_fr.values())\n",
        "  \n",
        "  for en_word, fr_word in en_fr.items():\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "      # get the english embedding\n",
        "\n",
        "      en_vec = english_vecs[en_word]\n",
        "\n",
        "      # get the french embedding\n",
        "      fr_vec = french_vecs[fr_word]\n",
        "\n",
        "      # add the english embedding to the list\n",
        "      X_l.append(en_vec)\n",
        "\n",
        "      # add the frencg embedding to the list\n",
        "      Y_l.append(fr_vec)\n",
        "\n",
        "  X = np.stack(X_l, axis=0)\n",
        "  Y = np.stack(Y_l, axis=0)\n",
        "\n",
        "  return X,Y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DEdoUYdya0e"
      },
      "source": [
        "# use get_matrices to obtain X_train and Y_train of english and french word embeddings into the corresponding vector space models\n",
        "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czkyzy6h115c",
        "outputId": "c4bcba35-fb82-4ee3-ffd7-8e4a05357af2"
      },
      "source": [
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'Y_train shape: {Y_train.shape}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (4932, 300)\n",
            "Y_train shape: (4932, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpaS1jow3rkX"
      },
      "source": [
        "## Translation as linear transformation of embeddings\n",
        "\n",
        "Given dictionaries of English and French word embeddings you will create a transformation matrix `R`\n",
        "* Given an English word embedding, $\\mathbf{e}$, you can multiply $\\mathbf{eR}$ to get a new word embedding $\\mathbf{f}$.\n",
        "    * Both $\\mathbf{e}$ and $\\mathbf{f}$ are row vectors\n",
        "* You can then compute the nearest neighbors to `f` in the french embeddings and recommend the word that is most similar to the transformed word embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rvmHtrb34nb"
      },
      "source": [
        "### Describing translation as the minimization problem\n",
        "\n",
        "Find a matrix `R` that minimizes the following equation. \n",
        "\n",
        "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F} $$\n",
        "\n",
        "### Frobenius norm\n",
        "\n",
        "The Frobenius norm of a matrix $A$ (assuming it is of dimension $m,n$) is defined as the square root of the sum of the absolute squares of its elements:\n",
        "\n",
        "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0-wGMif4CDe"
      },
      "source": [
        "### Actual loss function\n",
        "In the real world applications, the Frobenius norm loss:\n",
        "\n",
        "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
        "\n",
        "is often replaced by it's squared value divided by $m$:\n",
        "\n",
        "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
        "\n",
        "where $m$ is the number of examples (rows in $\\mathbf{X}$).\n",
        "\n",
        "* The same R is found when using this loss function versus the original Frobenius norm.\n",
        "* The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius.\n",
        "* The reason for dividing by $m$ is that we're more interested in the average loss per embedding than the  loss for the entire training set.\n",
        "    * The loss for all training set increases with more words (training examples),\n",
        "    so taking the average helps us to track the average loss regardless of the size of the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqDxExlH4s5M"
      },
      "source": [
        "#### Step 1: Computing the loss\n",
        "* The loss function will be squared Frobenoius norm of the difference between\n",
        "matrix and its approximation, divided by the number of training examples $m$.\n",
        "* Its formula is:\n",
        "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
        "\n",
        "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN23fy2Q3dmq"
      },
      "source": [
        "def compute_loss(X,Y,R):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    X: a matrix of dimension (mxn) where the columns are the english embeddings\n",
        "    Y: a matrix of dimension (mxn) where the columns are the french embeddings\n",
        "    R: a matrix of dimension (nxn) - transformation matrix from english to french vector space embeddings\n",
        "  Outputs:\n",
        "    loss: a scalar value representing loss\n",
        "  \"\"\"\n",
        "  m = X.shape[0]\n",
        "\n",
        "  # XR - Y\n",
        "  diff = np.dot(X,R) - Y\n",
        "\n",
        "  # element-wise square of the difference\n",
        "  diff_squared = np.square(diff)\n",
        "\n",
        "  # sum of the squared elements\n",
        "  sum_diff_squared = np.sum(diff_squared)\n",
        "\n",
        "  # divide by number of examples\n",
        "  loss = sum_diff_squared / m\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOHwjAYD6xEf"
      },
      "source": [
        "### Step 2: Computing the gradient of loss in respect to transform matrix R\n",
        "\n",
        "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
        "* The gradient is a matrix that encodes how much a small change in `R`\n",
        "affect the change in the loss function.\n",
        "* The gradient gives us the direction in which we should decrease `R`\n",
        "to minimize the loss.\n",
        "* $m$ is the number of training examples (number of rows in $X$).\n",
        "* The formula for the gradient of the loss function $ùêø(ùëã,ùëå,ùëÖ)$ is:\n",
        "\n",
        "$$\\frac{d}{dR}ùêø(ùëã,ùëå,ùëÖ)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig1MZfuy5grG"
      },
      "source": [
        "def compute_gradient(X,Y,R):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    X: a matrix of dimension (mxn) where the columns are the english embeddings\n",
        "    Y: a matrix of dimension (mxn) where the columns are the french embeddings\n",
        "    R: a matrix of dimension (nxn) - transformation matrix from english to french vector space embeddings\n",
        "  Outputs:\n",
        "    grad: a scalar value - gradient of the loss function L for given X,Y,R\n",
        "  \"\"\"\n",
        "  m = X.shape[0]\n",
        "\n",
        "  grad = X.T @ (X @ R - Y) * 2/m\n",
        "  return grad"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIfkHIcX8BUP"
      },
      "source": [
        "### Step 3: Finding the optimal R with gradient descent algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdtnB3z7-2P"
      },
      "source": [
        "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
        "    \"\"\"\"\n",
        "    Inputs:\n",
        "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
        "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
        "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
        "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
        "    Outputs:\n",
        "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
        "    \"\"\"\n",
        "    np.random.seed(129)\n",
        "\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if i % 25 == 0:\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        \n",
        "        gradient = compute_gradient(X,Y,R)\n",
        "\n",
        "        # update R by subtracting the learning rate times gradient\n",
        "        R -= learning_rate * gradient\n",
        "    return R"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI7XXA1B9WvX",
        "outputId": "3b847ba9-9014-4b40-94e0-147e875e5cd7"
      },
      "source": [
        "# Testing the function on random data\n",
        "np.random.seed(129)\n",
        "m = 10\n",
        "n = 5\n",
        "X = np.random.rand(m, n)\n",
        "Y = np.random.rand(m, n) * .1\n",
        "R = align_embeddings(X, Y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss at iteration 0 is: 3.7242\n",
            "loss at iteration 25 is: 3.6283\n",
            "loss at iteration 50 is: 3.5350\n",
            "loss at iteration 75 is: 3.4442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DmL6G9t9jpz"
      },
      "source": [
        "## Calculate transformation matrix R"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72zlAv3r9aZO",
        "outputId": "b3715e96-7727-4f7f-d810-22bc6a086b7b"
      },
      "source": [
        "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss at iteration 0 is: 963.0146\n",
            "loss at iteration 25 is: 97.8292\n",
            "loss at iteration 50 is: 26.8329\n",
            "loss at iteration 75 is: 9.7893\n",
            "loss at iteration 100 is: 4.3776\n",
            "loss at iteration 125 is: 2.3281\n",
            "loss at iteration 150 is: 1.4480\n",
            "loss at iteration 175 is: 1.0338\n",
            "loss at iteration 200 is: 0.8251\n",
            "loss at iteration 225 is: 0.7145\n",
            "loss at iteration 250 is: 0.6534\n",
            "loss at iteration 275 is: 0.6185\n",
            "loss at iteration 300 is: 0.5981\n",
            "loss at iteration 325 is: 0.5858\n",
            "loss at iteration 350 is: 0.5782\n",
            "loss at iteration 375 is: 0.5735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw8dPr8p-nj4"
      },
      "source": [
        "## Testing the translation using k-Nearest Neighbors\n",
        "\n",
        "### k-Nearest neighbors algorithm\n",
        "\n",
        "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) \n",
        "* k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. \n",
        "* The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors).\n",
        "\n",
        "### Searching for the translation embedding\n",
        "Since we're approximating the translation function from English to French embeddings by a linear transformation matrix $\\mathbf{R}$, most of the time we won't get the exact embedding of a French word when we transform embedding $\\mathbf{e}$ of some particular English word into the French embedding space. \n",
        "* This is where $k$-NN becomes really useful! By using $1$-NN with $\\mathbf{eR}$ as input, we can search for an embedding $\\mathbf{f}$ (as a row) in the matrix $\\mathbf{Y}$ which is the closest to the transformed vector $\\mathbf{eR}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpxTEaLp_EQW"
      },
      "source": [
        "### Cosine similarity\n",
        "Cosine similarity between vectors $u$ and $v$ calculated as the cosine of the angle between them.\n",
        "The formula is \n",
        "\n",
        "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
        "\n",
        "* $\\cos(u,v)$ = $1$ when $u$ and $v$ lie on the same line and have the same direction.\n",
        "* $\\cos(u,v)$ is $-1$ when they have exactly opposite directions.\n",
        "* $\\cos(u,v)$ is $0$ when the vectors are orthogonal (perpendicular) to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gYmekAW_LIu"
      },
      "source": [
        "#### Note: Distance and similarity are pretty much opposite things.\n",
        "* We can obtain distance metric from cosine similarity, but the cosine similarity can't be used directly as the distance metric. \n",
        "* When the cosine similarity increases (towards $1$), the \"distance\" between the two vectors decreases (towards $0$). \n",
        "* We can define the cosine distance between $u$ and $v$ as\n",
        "$$d_{\\text{cos}}(u,v)=1-\\cos(u,v)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3nC0Tvh-EHS"
      },
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    v: the vector to find the nearest neighbor for\n",
        "    candidates: a set of vectors where we find the neighbors\n",
        "    k: top k nearest neighbors to find\n",
        "  Output:\n",
        "    k_idx: indices of the top k closest vectors in sorted form\n",
        "  \"\"\"\n",
        "  similarity_l = []\n",
        "  for row in candidates:\n",
        "    # get the cosing similarity\n",
        "    cos_similarity = cosine_similarity(v, row)\n",
        "    similarity_l.append(cos_similarity)\n",
        "\n",
        "  \n",
        "  sorted_ids = np.argsort(similarity_l)[::-1]\n",
        "  k_idx = sorted_ids[:k]\n",
        "  k_idx = np.flip(k_idx)\n",
        "  return k_idx"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLRyvL5lBryp",
        "outputId": "bd993add-e02f-472e-9d0e-df76219bb854"
      },
      "source": [
        "# Test the implementation:\n",
        "v = np.array([1, 0, 1])\n",
        "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
        "print(candidates[nearest_neighbor(v, candidates, 1)])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here [2]\n",
            "here [2]\n",
            "[[2 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw5Gf0KaAw9z"
      },
      "source": [
        "## Testing the translation and comuting its accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGLQoNTaBydl"
      },
      "source": [
        "def test_vocabulary(X, Y, R):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      X: a matrix where the columns are the English embeddings.\n",
        "      Y: a matrix where the columns correspong to the French embeddings.\n",
        "      R: the transform matrix which translates word embeddings from English to French word vector space.\n",
        "  Output:\n",
        "      accuracy: for the English to French capitals\n",
        "  \"\"\"\n",
        "  pred = X @ R\n",
        "\n",
        "  num_correct = 0\n",
        "  for i in range(len(pred)):\n",
        "    pred_idx = nearest_neighbor(pred[i], Y, k=1)\n",
        "    if pred_idx == i:\n",
        "      num_correct += 1\n",
        "  accuracy = num_correct / len(pred)\n",
        "  return accuracy"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX25p260CPgJ"
      },
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdpFHLd1CXmZ",
        "outputId": "e1abc9a4-5c3b-43de-81ab-7c0753d4c1e9"
      },
      "source": [
        "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on test set is 0.557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zk0yQdnCZY6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}