{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Sentiment_Analysis_Using_Naive_Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeW02OWBruv8"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivRWrpkEh5rT"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjgam_fFrw9r",
        "outputId": "f49962ae-6203-4ac1-86dc-4f79055e2d51"
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ip5pGyMr0n-",
        "outputId": "9dce3900-debf-4457-b2a9-89fdadb890e1"
      },
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "print(f'Number of positive tweets: {len(all_positive_tweets)}')\n",
        "print(f'Number of negative tweets: {len(all_negative_tweets)}')\n",
        "\n",
        "print(f'Type of all positive tweets: {type(all_positive_tweets)}')\n",
        "print(f'Type of a tweet entry: {type(all_positive_tweets[0])}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive tweets: 5000\n",
            "Number of negative tweets: 5000\n",
            "Type of all positive tweets: <class 'list'>\n",
            "Type of a tweet entry: <class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0QTBw-vr-Fq"
      },
      "source": [
        "## Preprocess tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlyqCk8kr53Q"
      },
      "source": [
        "# Let's make a function to process a give tweet \n",
        "def process_tweet(tweet):\n",
        "  # Remove retweets, hyperlinks, hashtag sign\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  # tokenize\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  # remove stopwords and punctuations\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  tweets_clean = []\n",
        "\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "      tweets_clean.append(word)\n",
        "\n",
        "  # stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  tweets_stem = []\n",
        "\n",
        "  for word in tweets_clean:\n",
        "    stem_word = stemmer.stem(word)\n",
        "    tweets_stem.append(stem_word)\n",
        "\n",
        "  return tweets_stem"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B9FFcYmsKGR"
      },
      "source": [
        "## Build word frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTbRJxkbsA6z"
      },
      "source": [
        "# build freq dictionary\n",
        "def build_freqs(tweets, labels):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    tweets: a list of tweets\n",
        "    labels: an mx1 array with the sentiment label of each tweet (1 or 0)\n",
        "  Output:\n",
        "    freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n",
        "  \"\"\"\n",
        "\n",
        "  labels_list = np.squeeze(labels).tolist()\n",
        "\n",
        "  freqs = {}\n",
        "  for tweet, label in zip(tweets, labels_list):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, label)\n",
        "      freqs[pair] = freqs.get(pair, 0) + 1\n",
        "  \n",
        "  return freqs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNRCAF1ssTr4"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7xnkeNfsQnt"
      },
      "source": [
        "# split the data into two pieces, one for training and one for testing\n",
        "# 80% training set, 20% test set\n",
        "\n",
        "split = 0.8\n",
        "pos_size = (int)(len(all_positive_tweets) * split)\n",
        "train_pos = all_positive_tweets[:pos_size]\n",
        "test_pos = all_positive_tweets[pos_size:]\n",
        "\n",
        "train_neg = all_negative_tweets[:pos_size]\n",
        "test_neg = all_negative_tweets[pos_size:]\n",
        "\n",
        "X_train = train_pos + train_neg\n",
        "X_test = test_pos + test_neg\n",
        "\n",
        "# combine positve and negative labels\n",
        "y_train = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "y_test = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk1ff7hVtbq1",
        "outputId": "dbdd8600-2de6-4cac-a910-374105c0c28a"
      },
      "source": [
        "print(len(X_train))\n",
        "print(y_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "(8000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exT-eA12sfhf",
        "outputId": "6ffa1ffd-5e69-4a14-ba5a-d478bd414ccd"
      },
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(X_train, y_train)\n",
        "print(f'len(freqs): {str(len(freqs.keys()))}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(freqs): 11345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnUUWCSN2Ulu"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoNaWvAZsj8A"
      },
      "source": [
        "def train_naive_bayes(tweets, labels, freqs, num_classes=2):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    tweets: a list of tweets\n",
        "    labels: mx1 array of labels\n",
        "    freqs: word frequencies\n",
        "    num_classes: total number of classes (here we have positive label (1) and negative label (0))\n",
        "\n",
        "  Output:\n",
        "    logprior[c]: log of priors of each class i.e log P(c)\n",
        "    loglikelihood[w,c]: log likelihood of each class i.e log P(w|c)\n",
        "  \"\"\"\n",
        "  V = len(freqs)\n",
        "  logprior = {}\n",
        "  loglikelihood = {}\n",
        "\n",
        "  labels_list = labels.squeeze().tolist()\n",
        "\n",
        "  N_doc = len(tweets)\n",
        "\n",
        "  # calculating log prior\n",
        "  for c in range(num_classes): \n",
        "    N_c = 0\n",
        "    for tweet, label in zip(tweets, labels_list):\n",
        "      N_c += label\n",
        "    logprior[c] = np.log(N_c / N_doc)\n",
        "        \n",
        "\n",
        "  count_c = np.zeros(V)\n",
        "  for pair, count in freqs.items():\n",
        "    count_c[int(pair[1])] += count\n",
        "  \n",
        "  # calculating log likelihood\n",
        "  for pair, count in freqs.items():\n",
        "    loglikelihood[pair] = np.log( (count + 1) / (count_c[int(pair[1])] + 1) )\n",
        "\n",
        "  return logprior, loglikelihood"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kUowi1YzVwC"
      },
      "source": [
        "def test_naive_bayes(X_test, y_test, logprior, loglikelihood, num_classes=2):\n",
        "\n",
        "  sum = np.zeros((len(X_test) , num_classes))\n",
        "  for i in range(len(X_test)):\n",
        "    tweet = X_test[i]\n",
        "    for c in range(num_classes):\n",
        "      sum[i,c] = logprior[c]\n",
        "      for word in process_tweet(tweet):\n",
        "        sum[i,c] += loglikelihood.get((word,c), 0)\n",
        "\n",
        "  y_pred = np.argmax(sum, axis=1)\n",
        "  preds = (y_pred == y_test.squeeze()).sum()\n",
        "\n",
        "  accuracy = preds / len(y_test)\n",
        "  return accuracy"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPF0fJhwv1Yo"
      },
      "source": [
        "logprior, loglikelihood = train_naive_bayes(X_train, y_train, freqs)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7qbz02Lxy_x",
        "outputId": "0abfd673-ab0e-4ae3-87bd-6a6b32f141ae"
      },
      "source": [
        "accuracy = test_naive_bayes(X_test, y_test, logprior, loglikelihood)\n",
        "print(f'Naive bayes accuracy: {accuracy}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive bayes accuracy: 0.683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZaVVC5u28qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}