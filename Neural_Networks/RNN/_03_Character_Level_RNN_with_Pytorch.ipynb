{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_03_Character_Level_RNN_with_Pytorch.ipynb",
      "provenance": [],
      "mount_file_id": "1ZdWXcCFRDEFVpItRY7VRTHPevqWl06iC",
      "authorship_tag": "ABX9TyPyYjsv/9HCPPofjI5WGGnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palash04/Artificial-Intelligence/blob/master/Neural_Networks/RNN/_03_Character_Level_RNN_with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbGGDLhmDuxi",
        "colab_type": "text"
      },
      "source": [
        "# Character Level LSTM\n",
        "\n",
        "#### The network will train character by character on some text, then generate new text character by character. As an example, we will train on Anna Karenina. This model will be able to generate new text based on the text from the book!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8_RxtvXD_HF",
        "colab_type": "text"
      },
      "source": [
        "## Genreral Architecture of character-wise RNN\n",
        "![Screenshot 2020-07-18 at 18 44 33](https://user-images.githubusercontent.com/26361028/87853322-b9b7eb80-c926-11ea-95b7-693375822bde.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhDTVRjXwMyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = '/content/drive/My Drive/Artificial Intelligence/DataSet/RNN/anna.txt'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF9rHY0tERC5",
        "colab_type": "text"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY5ikDb5DbC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLAxBVfsEivb",
        "colab_type": "text"
      },
      "source": [
        "## Load in data\n",
        "\n",
        "### Load the Anna Karenina text file and convert it into integers for our networks to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MeOg-bfEbFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open text file and read in data as text\n",
        "with open(datapath,'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM1ucrFAFE2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91c2be00-f86e-46da-9a6c-a74dbd040426"
      },
      "source": [
        "# checking the first 100 characters\n",
        "text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYiu1OoCF00c",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwWnLs5dFSKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in \n",
        "# the network\n",
        "\n",
        "# Encode the text and map each character to an integer and vice versa\n",
        "# Creating dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii,ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al9Zqz1uHD37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93fe87c8-6e94-4ac1-8307-7c6594acf8f1"
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojMew_wbHFSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "904128b8-de49-4410-c5d5-1297a8fac463"
      },
      "source": [
        "int2char[82]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFIIK35qHqX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02e9700b-85be-4ebc-b3c1-cb7df02d0dd1"
      },
      "source": [
        "char2int['P']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZDg6ZF2Hu-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "695938dd-5f30-4ff9-8ccb-b2283c55f83d"
      },
      "source": [
        "# Let's see the first 100 encoded characters\n",
        "encoded[:100]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([71, 34, 68, 81, 74, 21, 82, 41, 42, 63, 63, 63, 79, 68, 81, 81, 55,\n",
              "       41, 36, 68, 50, 72, 56, 72, 21, 60, 41, 68, 82, 21, 41, 68, 56, 56,\n",
              "       41, 68, 56, 72, 26, 21, 19, 41, 21, 70, 21, 82, 55, 41, 75,  0, 34,\n",
              "       68, 81, 81, 55, 41, 36, 68, 50, 72, 56, 55, 41, 72, 60, 41, 75,  0,\n",
              "       34, 68, 81, 81, 55, 41, 72,  0, 41, 72, 74, 60, 41, 40, 14,  0, 63,\n",
              "       14, 68, 55,  6, 63, 63,  3, 70, 21, 82, 55, 74, 34, 72,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Hc5r_-H0r1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f70146fc-a028-4a1b-88ce-4708e4f46a31"
      },
      "source": [
        "print (int2char[12])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9GqLovLM2Fc",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the data\n",
        "As you can see in our char-RNN image above, our LSTM expects an input that is one-hot encoded meaning that each character is converted into an integer (via our created dictionary) and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since we're one-hot encoding the data, let's make a function to do that!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khbTDwWSH5N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  # Initialize the encoded array\n",
        "  one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "  # Fill the appropriate elements with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "  # Finally reshape it to get back to the original array\n",
        "  one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
        "\n",
        "  return one_hot"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6URb_ABN7O3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "756b7674-166f-4030-891b-b573cca7643f"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3,5,1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print (one_hot)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSiDEDoRRFSq",
        "colab_type": "text"
      },
      "source": [
        "## Making training mini-batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH4kELUFRYvL",
        "colab_type": "text"
      },
      "source": [
        "![Screenshot 2020-07-18 at 19 42 34](https://user-images.githubusercontent.com/26361028/87854322-d526f480-c92e-11ea-86ea-0b5334178969.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMSSYviONX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  batch_size_total = batch_size * seq_length\n",
        "  # total number of batches we can make\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "\n",
        "  # keep only enough characters to make full batches\n",
        "  arr = arr[:n_batches * batch_size_total]\n",
        "  # reshape into batch size rows\n",
        "  arr = arr.reshape((batch_size,-1))\n",
        "\n",
        "  # iterate through the array, one sequence at a time\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    # The features\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    # The targets, shifted by one\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzYWi_EmSoZY",
        "colab_type": "text"
      },
      "source": [
        "### Testing the implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_SSToLOOSvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x,y = next(batches)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq3jL3gLS24v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "938df081-7635-4ec3-85eb-67397c0aaf84"
      },
      "source": [
        "# Printing out the first 10 items in a sequence\n",
        "print ('x\\n', x[:10,:10])\n",
        "print ('\\ny\\n',y[:10,:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[71 34 68 81 74 21 82 41 42 63]\n",
            " [60 40  0 41 74 34 68 74 41 68]\n",
            " [21  0 61 41 40 82 41 68 41 36]\n",
            " [60 41 74 34 21 41  8 34 72 21]\n",
            " [41 60 68 14 41 34 21 82 41 74]\n",
            " [ 8 75 60 60 72 40  0 41 68  0]\n",
            " [41 29  0  0 68 41 34 68 61 41]\n",
            " [78 44 56 40  0 60 26 55  6 41]]\n",
            "\n",
            "y\n",
            " [[34 68 81 74 21 82 41 42 63 63]\n",
            " [40  0 41 74 34 68 74 41 68 74]\n",
            " [ 0 61 41 40 82 41 68 41 36 40]\n",
            " [41 74 34 21 41  8 34 72 21 36]\n",
            " [60 68 14 41 34 21 82 41 74 21]\n",
            " [75 60 60 72 40  0 41 68  0 61]\n",
            " [29  0  0 68 41 34 68 61 41 60]\n",
            " [44 56 40  0 60 26 55  6 41 49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS6zSEaBHHHy",
        "colab_type": "text"
      },
      "source": [
        "# Defining the layer wih pytorch\n",
        "![Screenshot 2020-07-19 at 08 56 05](https://user-images.githubusercontent.com/26361028/87866295-b0b63100-c99d-11ea-908a-a2c4d8e1e57a.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCLsAduLHOAh",
        "colab_type": "text"
      },
      "source": [
        "### Model Structure - \n",
        "In __ _init___ the suggested structure is as follows - \n",
        "- Create and store the necessary dictionaries\n",
        "- Define the LSTM layer which takes as params: an input size(the number of characters), a hidden layer size(n_hidden), a number of layers(n_layers), a dropout probability (drop_prob), and a batch first boolean (True, since we are batching).\n",
        "- Define a dropout layer with dropout prob\n",
        "- Define a fully connected layer with params: input size, n_hidden, and output size (the number of characters).\n",
        "- Finally initialize the weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiblZqQhIra_",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Inputs/Outputs\n",
        "Basic LSTM layer looks like as follows:\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, dropout=drop_prob,batch_first=True)\n",
        "\n",
        "where input_size is the number of characters this cell expects to see as sequential input, and n_hidden is the number of units in the hidden layers in the cell. And we can add dropout by adding a dropout parameter with a specified probability; this will automatically add dropout to the inputs or outputs. Finally, in the forward function, we can stack up the LSTM cells into layers using .view. With this, you pass in a list of cells and it will send the output of one cell into the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRh1isdZS2KU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d19c7922-bb43-4da2-db59-5508d52eaedc"
      },
      "source": [
        "# Check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "  print ('Training on gpu')\n",
        "else:\n",
        "  print ('No gpu available, training on cpu. Consider making n_epochs very small')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ucrz6LHKD4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden = 256, n_layers = 2, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    # creating character dictionaries\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch : ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    # Define the LSTM\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first = True)\n",
        "\n",
        "    # Define a dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    # Define the final, fully connected output layer\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    # Get the outputs and the new hidden state from the lstm\n",
        "    r_output, hidden = self.lstm(x,hidden)\n",
        "\n",
        "    # pass through a dropout layer\n",
        "    out = self.dropout(r_output)\n",
        "\n",
        "    # Stack up LSTM outputs\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "    # put out throght fully connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    # Initialize hidden state\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if train_on_gpu:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    return hidden"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5g__964RuzV",
        "colab_type": "text"
      },
      "source": [
        "# Train the Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HlhDVsKRtG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs = 10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1,print_every=10):\n",
        "  ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "  '''\n",
        "  net.train()\n",
        "  opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Create training and validation data\n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "  if train_on_gpu:\n",
        "    net.cuda()\n",
        "  \n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for x,y in get_batches(data, batch_size, seq_length):\n",
        "      counter += 1\n",
        "\n",
        "      # One hot encode our data, and make them torch Tensors\n",
        "      x = one_hot_encode(x, n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "      if train_on_gpu:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      # zero accumulated gradients\n",
        "      net.zero_grad()\n",
        "\n",
        "      # get the output from the model\n",
        "      output, h = net(inputs, h)\n",
        "\n",
        "      # calculate the loss and perform backprop\n",
        "      loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "      loss.backward()\n",
        "      # clip_grad_norm helps prevent the exploding gradient problem in RNN / LSTM\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "      opt.step()\n",
        "    \n",
        "      # loss stats\n",
        "      if counter % print_every == 0:\n",
        "        # Get validation loss\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "            \n",
        "            inputs, targets = x, y\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            output, val_h = net(inputs, val_h)\n",
        "            val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        \n",
        "            val_losses.append(val_loss.item())\n",
        "        net.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2dM2qPrVUhq",
        "colab_type": "text"
      },
      "source": [
        "## Instantiating the model\n",
        "Now we can actually train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q3lRaWxVTtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d9bd5944-a8d3-4305-89f6-2b3702172372"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print (net)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IStRm3lGVlbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0559699-18fb-4df3-8f72-3fed506b1204"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20   # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2708... Val Loss: 3.1814\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1521... Val Loss: 3.1302\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1445... Val Loss: 3.1204\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1154... Val Loss: 3.1186\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1415... Val Loss: 3.1181\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1162... Val Loss: 3.1160\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1115... Val Loss: 3.1152\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1234... Val Loss: 3.1129\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1242... Val Loss: 3.1078\n",
            "Epoch: 1/20... Step: 100... Loss: 3.1074... Val Loss: 3.1001\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0949... Val Loss: 3.0815\n",
            "Epoch: 1/20... Step: 120... Loss: 3.0433... Val Loss: 3.0400\n",
            "Epoch: 1/20... Step: 130... Loss: 3.0003... Val Loss: 2.9717\n",
            "Epoch: 2/20... Step: 140... Loss: 2.9444... Val Loss: 2.9105\n",
            "Epoch: 2/20... Step: 150... Loss: 2.8438... Val Loss: 2.7852\n",
            "Epoch: 2/20... Step: 160... Loss: 2.7475... Val Loss: 2.6990\n",
            "Epoch: 2/20... Step: 170... Loss: 2.6432... Val Loss: 2.6331\n",
            "Epoch: 2/20... Step: 180... Loss: 2.6335... Val Loss: 2.5934\n",
            "Epoch: 2/20... Step: 190... Loss: 2.5620... Val Loss: 2.5522\n",
            "Epoch: 2/20... Step: 200... Loss: 2.5535... Val Loss: 2.5073\n",
            "Epoch: 2/20... Step: 210... Loss: 2.5136... Val Loss: 2.4788\n",
            "Epoch: 2/20... Step: 220... Loss: 2.4771... Val Loss: 2.4504\n",
            "Epoch: 2/20... Step: 230... Loss: 2.4545... Val Loss: 2.4212\n",
            "Epoch: 2/20... Step: 240... Loss: 2.4461... Val Loss: 2.4010\n",
            "Epoch: 2/20... Step: 250... Loss: 2.3841... Val Loss: 2.3741\n",
            "Epoch: 2/20... Step: 260... Loss: 2.3658... Val Loss: 2.3552\n",
            "Epoch: 2/20... Step: 270... Loss: 2.3643... Val Loss: 2.3308\n",
            "Epoch: 3/20... Step: 280... Loss: 2.3733... Val Loss: 2.3171\n",
            "Epoch: 3/20... Step: 290... Loss: 2.3312... Val Loss: 2.2886\n",
            "Epoch: 3/20... Step: 300... Loss: 2.3152... Val Loss: 2.2674\n",
            "Epoch: 3/20... Step: 310... Loss: 2.2963... Val Loss: 2.2571\n",
            "Epoch: 3/20... Step: 320... Loss: 2.2639... Val Loss: 2.2274\n",
            "Epoch: 3/20... Step: 330... Loss: 2.2245... Val Loss: 2.2102\n",
            "Epoch: 3/20... Step: 340... Loss: 2.2314... Val Loss: 2.1897\n",
            "Epoch: 3/20... Step: 350... Loss: 2.2282... Val Loss: 2.1745\n",
            "Epoch: 3/20... Step: 360... Loss: 2.1686... Val Loss: 2.1635\n",
            "Epoch: 3/20... Step: 370... Loss: 2.1874... Val Loss: 2.1411\n",
            "Epoch: 3/20... Step: 380... Loss: 2.1567... Val Loss: 2.1274\n",
            "Epoch: 3/20... Step: 390... Loss: 2.1527... Val Loss: 2.1080\n",
            "Epoch: 3/20... Step: 400... Loss: 2.1138... Val Loss: 2.0935\n",
            "Epoch: 3/20... Step: 410... Loss: 2.1202... Val Loss: 2.0775\n",
            "Epoch: 4/20... Step: 420... Loss: 2.0932... Val Loss: 2.0615\n",
            "Epoch: 4/20... Step: 430... Loss: 2.0959... Val Loss: 2.0457\n",
            "Epoch: 4/20... Step: 440... Loss: 2.0756... Val Loss: 2.0330\n",
            "Epoch: 4/20... Step: 450... Loss: 2.0083... Val Loss: 2.0173\n",
            "Epoch: 4/20... Step: 460... Loss: 2.0176... Val Loss: 2.0072\n",
            "Epoch: 4/20... Step: 470... Loss: 2.0354... Val Loss: 1.9959\n",
            "Epoch: 4/20... Step: 480... Loss: 2.0122... Val Loss: 1.9897\n",
            "Epoch: 4/20... Step: 490... Loss: 2.0177... Val Loss: 1.9717\n",
            "Epoch: 4/20... Step: 500... Loss: 2.0183... Val Loss: 1.9593\n",
            "Epoch: 4/20... Step: 510... Loss: 1.9867... Val Loss: 1.9452\n",
            "Epoch: 4/20... Step: 520... Loss: 2.0134... Val Loss: 1.9388\n",
            "Epoch: 4/20... Step: 530... Loss: 1.9572... Val Loss: 1.9267\n",
            "Epoch: 4/20... Step: 540... Loss: 1.9196... Val Loss: 1.9128\n",
            "Epoch: 4/20... Step: 550... Loss: 1.9703... Val Loss: 1.9056\n",
            "Epoch: 5/20... Step: 560... Loss: 1.9367... Val Loss: 1.8942\n",
            "Epoch: 5/20... Step: 570... Loss: 1.9105... Val Loss: 1.8831\n",
            "Epoch: 5/20... Step: 580... Loss: 1.8900... Val Loss: 1.8794\n",
            "Epoch: 5/20... Step: 590... Loss: 1.9010... Val Loss: 1.8629\n",
            "Epoch: 5/20... Step: 600... Loss: 1.8821... Val Loss: 1.8541\n",
            "Epoch: 5/20... Step: 610... Loss: 1.8689... Val Loss: 1.8499\n",
            "Epoch: 5/20... Step: 620... Loss: 1.8621... Val Loss: 1.8395\n",
            "Epoch: 5/20... Step: 630... Loss: 1.8806... Val Loss: 1.8282\n",
            "Epoch: 5/20... Step: 640... Loss: 1.8557... Val Loss: 1.8178\n",
            "Epoch: 5/20... Step: 650... Loss: 1.8335... Val Loss: 1.8103\n",
            "Epoch: 5/20... Step: 660... Loss: 1.8092... Val Loss: 1.8021\n",
            "Epoch: 5/20... Step: 670... Loss: 1.8449... Val Loss: 1.7945\n",
            "Epoch: 5/20... Step: 680... Loss: 1.8361... Val Loss: 1.7882\n",
            "Epoch: 5/20... Step: 690... Loss: 1.8077... Val Loss: 1.7812\n",
            "Epoch: 6/20... Step: 700... Loss: 1.8014... Val Loss: 1.7707\n",
            "Epoch: 6/20... Step: 710... Loss: 1.8004... Val Loss: 1.7618\n",
            "Epoch: 6/20... Step: 720... Loss: 1.7743... Val Loss: 1.7557\n",
            "Epoch: 6/20... Step: 730... Loss: 1.7960... Val Loss: 1.7436\n",
            "Epoch: 6/20... Step: 740... Loss: 1.7566... Val Loss: 1.7393\n",
            "Epoch: 6/20... Step: 750... Loss: 1.7309... Val Loss: 1.7353\n",
            "Epoch: 6/20... Step: 760... Loss: 1.7692... Val Loss: 1.7254\n",
            "Epoch: 6/20... Step: 770... Loss: 1.7542... Val Loss: 1.7170\n",
            "Epoch: 6/20... Step: 780... Loss: 1.7373... Val Loss: 1.7142\n",
            "Epoch: 6/20... Step: 790... Loss: 1.7348... Val Loss: 1.7061\n",
            "Epoch: 6/20... Step: 800... Loss: 1.7316... Val Loss: 1.7033\n",
            "Epoch: 6/20... Step: 810... Loss: 1.7253... Val Loss: 1.6930\n",
            "Epoch: 6/20... Step: 820... Loss: 1.6974... Val Loss: 1.6896\n",
            "Epoch: 6/20... Step: 830... Loss: 1.7308... Val Loss: 1.6805\n",
            "Epoch: 7/20... Step: 840... Loss: 1.6869... Val Loss: 1.6735\n",
            "Epoch: 7/20... Step: 850... Loss: 1.6990... Val Loss: 1.6683\n",
            "Epoch: 7/20... Step: 860... Loss: 1.6824... Val Loss: 1.6622\n",
            "Epoch: 7/20... Step: 870... Loss: 1.6927... Val Loss: 1.6556\n",
            "Epoch: 7/20... Step: 880... Loss: 1.6865... Val Loss: 1.6514\n",
            "Epoch: 7/20... Step: 890... Loss: 1.6801... Val Loss: 1.6461\n",
            "Epoch: 7/20... Step: 900... Loss: 1.6517... Val Loss: 1.6385\n",
            "Epoch: 7/20... Step: 910... Loss: 1.6360... Val Loss: 1.6341\n",
            "Epoch: 7/20... Step: 920... Loss: 1.6614... Val Loss: 1.6312\n",
            "Epoch: 7/20... Step: 930... Loss: 1.6479... Val Loss: 1.6280\n",
            "Epoch: 7/20... Step: 940... Loss: 1.6412... Val Loss: 1.6252\n",
            "Epoch: 7/20... Step: 950... Loss: 1.6560... Val Loss: 1.6181\n",
            "Epoch: 7/20... Step: 960... Loss: 1.6466... Val Loss: 1.6124\n",
            "Epoch: 7/20... Step: 970... Loss: 1.6495... Val Loss: 1.6063\n",
            "Epoch: 8/20... Step: 980... Loss: 1.6298... Val Loss: 1.6051\n",
            "Epoch: 8/20... Step: 990... Loss: 1.6303... Val Loss: 1.6005\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.6173... Val Loss: 1.5916\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.6592... Val Loss: 1.5879\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.6155... Val Loss: 1.5901\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.6053... Val Loss: 1.5833\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.6040... Val Loss: 1.5783\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.5898... Val Loss: 1.5714\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.5978... Val Loss: 1.5682\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5954... Val Loss: 1.5666\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5954... Val Loss: 1.5639\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.5818... Val Loss: 1.5569\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.5669... Val Loss: 1.5530\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.5710... Val Loss: 1.5517\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.5891... Val Loss: 1.5482\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.5739... Val Loss: 1.5459\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.5729... Val Loss: 1.5364\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5895... Val Loss: 1.5368\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.5473... Val Loss: 1.5385\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.5519... Val Loss: 1.5303\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.5441... Val Loss: 1.5250\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.5747... Val Loss: 1.5195\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.5187... Val Loss: 1.5204\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.5269... Val Loss: 1.5194\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.5338... Val Loss: 1.5160\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.5115... Val Loss: 1.5112\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.5194... Val Loss: 1.5092\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.5264... Val Loss: 1.5054\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.5360... Val Loss: 1.5077\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.5255... Val Loss: 1.5063\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.5399... Val Loss: 1.4965\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.5213... Val Loss: 1.4984\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.5095... Val Loss: 1.4974\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.5149... Val Loss: 1.4925\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4888... Val Loss: 1.4865\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4963... Val Loss: 1.4840\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4755... Val Loss: 1.4827\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.4736... Val Loss: 1.4832\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4744... Val Loss: 1.4786\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.4717... Val Loss: 1.4759\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.5112... Val Loss: 1.4724\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.5049... Val Loss: 1.4716\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.5137... Val Loss: 1.4687\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.5219... Val Loss: 1.4663\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.5086... Val Loss: 1.4605\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4776... Val Loss: 1.4637\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.5018... Val Loss: 1.4619\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.4293... Val Loss: 1.4603\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.4598... Val Loss: 1.4563\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.4425... Val Loss: 1.4551\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.4718... Val Loss: 1.4551\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.4589... Val Loss: 1.4536\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.4380... Val Loss: 1.4488\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.4243... Val Loss: 1.4460\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.4626... Val Loss: 1.4438\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4985... Val Loss: 1.4429\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.4616... Val Loss: 1.4420\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.4676... Val Loss: 1.4396\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4712... Val Loss: 1.4353\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.4366... Val Loss: 1.4371\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3995... Val Loss: 1.4365\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3977... Val Loss: 1.4316\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.4311... Val Loss: 1.4331\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.4177... Val Loss: 1.4290\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.4223... Val Loss: 1.4293\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.4429... Val Loss: 1.4261\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.4089... Val Loss: 1.4248\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3854... Val Loss: 1.4201\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.4501... Val Loss: 1.4194\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.4093... Val Loss: 1.4192\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.4261... Val Loss: 1.4163\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.4009... Val Loss: 1.4121\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.4088... Val Loss: 1.4117\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3783... Val Loss: 1.4128\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3916... Val Loss: 1.4114\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.4350... Val Loss: 1.4077\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3942... Val Loss: 1.4088\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3634... Val Loss: 1.4063\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3980... Val Loss: 1.4047\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.4062... Val Loss: 1.4041\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3869... Val Loss: 1.4025\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3757... Val Loss: 1.3981\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.4038... Val Loss: 1.3991\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.4004... Val Loss: 1.4004\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3832... Val Loss: 1.3975\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3971... Val Loss: 1.3895\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.3505... Val Loss: 1.3912\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.3302... Val Loss: 1.3932\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3953... Val Loss: 1.3923\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3968... Val Loss: 1.3881\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3821... Val Loss: 1.3908\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.4021... Val Loss: 1.3870\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3692... Val Loss: 1.3844\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3884... Val Loss: 1.3866\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3723... Val Loss: 1.3838\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.3491... Val Loss: 1.3803\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.4021... Val Loss: 1.3817\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3555... Val Loss: 1.3897\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3620... Val Loss: 1.3815\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3591... Val Loss: 1.3774\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.3467... Val Loss: 1.3790\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.3523... Val Loss: 1.3776\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.3322... Val Loss: 1.3748\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.3542... Val Loss: 1.3718\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3772... Val Loss: 1.3763\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.3358... Val Loss: 1.3705\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.3527... Val Loss: 1.3692\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.3361... Val Loss: 1.3677\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.3473... Val Loss: 1.3693\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3593... Val Loss: 1.3659\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.3461... Val Loss: 1.3667\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.3457... Val Loss: 1.3720\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.3375... Val Loss: 1.3711\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.3352... Val Loss: 1.3608\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.3459... Val Loss: 1.3614\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.3155... Val Loss: 1.3619\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.3255... Val Loss: 1.3611\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3495... Val Loss: 1.3568\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.3369... Val Loss: 1.3602\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.3270... Val Loss: 1.3609\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.3175... Val Loss: 1.3564\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.3423... Val Loss: 1.3578\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.3226... Val Loss: 1.3540\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2844... Val Loss: 1.3509\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.3354... Val Loss: 1.3526\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.3059... Val Loss: 1.3578\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.3224... Val Loss: 1.3580\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.3006... Val Loss: 1.3504\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.3111... Val Loss: 1.3486\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.3270... Val Loss: 1.3514\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.3304... Val Loss: 1.3483\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.3198... Val Loss: 1.3457\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2855... Val Loss: 1.3460\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.3140... Val Loss: 1.3434\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.3007... Val Loss: 1.3415\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.3052... Val Loss: 1.3452\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.3134... Val Loss: 1.3446\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.3249... Val Loss: 1.3400\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.3227... Val Loss: 1.3403\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2977... Val Loss: 1.3472\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.3051... Val Loss: 1.3397\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.3009... Val Loss: 1.3374\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.3252... Val Loss: 1.3381\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.3095... Val Loss: 1.3377\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2927... Val Loss: 1.3355\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.3067... Val Loss: 1.3364\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2871... Val Loss: 1.3343\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2907... Val Loss: 1.3349\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.3051... Val Loss: 1.3330\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2988... Val Loss: 1.3346\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2933... Val Loss: 1.3314\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2779... Val Loss: 1.3307\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2819... Val Loss: 1.3305\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2965... Val Loss: 1.3301\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2987... Val Loss: 1.3336\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.3078... Val Loss: 1.3284\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.3251... Val Loss: 1.3267\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2792... Val Loss: 1.3281\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2916... Val Loss: 1.3236\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2848... Val Loss: 1.3232\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.3190... Val Loss: 1.3221\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2624... Val Loss: 1.3239\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2702... Val Loss: 1.3265\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2863... Val Loss: 1.3240\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2673... Val Loss: 1.3213\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2675... Val Loss: 1.3205\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2818... Val Loss: 1.3240\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2875... Val Loss: 1.3247\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2911... Val Loss: 1.3245\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.3033... Val Loss: 1.3198\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2843... Val Loss: 1.3169\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2692... Val Loss: 1.3203\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2847... Val Loss: 1.3178\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2600... Val Loss: 1.3155\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2515... Val Loss: 1.3143\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2476... Val Loss: 1.3162\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2458... Val Loss: 1.3150\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2546... Val Loss: 1.3142\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2490... Val Loss: 1.3139\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2813... Val Loss: 1.3125\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.3030... Val Loss: 1.3129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuUFA33PXaNT",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbMQXAlwWCSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = '/content/drive/My Drive/Artificial Intelligence/rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPOm-_h1XwEe",
        "colab_type": "text"
      },
      "source": [
        "# Making Predictions\n",
        "Now that the model is trained, we'll want to sample from it and make predictions about next characters! To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWjIZ-scXpjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZZCB-TYNxG",
        "colab_type": "text"
      },
      "source": [
        "### Priming and generating text\n",
        "\n",
        "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYN6hXiLYBXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obf2T95bYUZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "87531ceb-b0b4-4705-b314-b7d4c91a6ad8"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna Anna had always been at from them to say a charming, but there was no time that he had to be\n",
            "the stream of heart for this mind in a calition and horror. He husband had stopped in, and all the countess, she would have been faring this happiness of when that in the couries of sevent of the steps would not take the meeting attraction on her supporition of sent the criel, there were all\n",
            "the conversation that that would be to be so seeing that it was not in at the chain of them, and had been satisfied that there could not see him her face, that they was already a fine soul in his head.\"\n",
            "\n",
            "The more sorring, he could not help, the service were stood a little brisk, the sole waists and to be started over some sensing a sort in\n",
            "her. The portrait the same work\n",
            "and shirt that was the best a feeling of thought in, talked away at expression of his heart. They\n",
            "saw it to be feeling to be discussing the money, and went to a\n",
            "little cape, he writed\n",
            "the pleasant coat, she was not so interesting.\n",
            "\n",
            "She had \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE8xmqT-YfQ8",
        "colab_type": "text"
      },
      "source": [
        "## Loading a checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOyZ9p6YYYI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "042b3c95-8bd1-4189-e77e-540c21dd8acc"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open(model_name, 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O3iAU_BYlQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "19b7054a-a7aa-4e47-b512-87ea27cc8df3"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said to her\n",
            "the back of his father with the same chair of the pate, and had been before the serious sick and\n",
            "tried now and taken the people of the cross and almost,\n",
            "he was a found in her strange and that share of his feet that the position was not to dinner and heard the position, but all the same toothing was her tond of the fresh alone of the midst of all that there was an ordinary through something another face on the princess. Alexey\n",
            "Alexandrovitch chomed her hair, with her friend, had\n",
            "a perfect of the\n",
            "most crassed settle any people\n",
            "and her eyes\n",
            "were assertanily to she had not been atreed it, but so that he had to go on with horses, and his bottle waiting over the meetis antwer and the\n",
            "mastal, but was at all to have such a man to be still still so something there and their princismens. He had been discreated. But since he had breathing on his strangere standing\n",
            "to the same smare. \"What is a pity.\"\n",
            "\n",
            "She had been brought the more\n",
            "for him that she saw\n",
            "a believe in the\n",
            "most chair of the charm who always had no one. He was sticking at her side in this coming out where the setting sound at\n",
            "a soul to herself a child, he heard\n",
            "the passionants, to see them wound her and showed to this\n",
            "serene with her for this, he went on,\n",
            "her face trying to speak with a\n",
            "most coachman, and she saw\n",
            "him that in the same, and tulling the sight of all, which had been beginning and the solute through the same start of mind, an horricien of the first to make a patily and work to her husband, and to the position went to him the belution of his bottle which she\n",
            "had been dispased in the studding, and\n",
            "then to the mother was the same, the peace of his beauty were not fancied\n",
            "them of the pickous of a subject\n",
            "of his brother's forest to the side of the same. That had already talker of the members, but he could see that she had to be all her face, before he fancied he could not carried. But in\n",
            "the\n",
            "more, and at the presture, took over his feelings\n",
            "in his face that she had become taken his\n",
            "feeling was saying h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxfgMXvyYvS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "fab8a22a-121c-4b21-b41a-f0002039ab47"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=10, prime=\"God\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "God of side, when Levin were to dinner for finter her\n",
            "family, and had been not and all the way, till the carriage, and\n",
            "her hard their course of the standing to cress on the wild. But outside\n",
            "in society that he had\n",
            "telled the bivelable of the strange.\n",
            "And her most doge, and\n",
            "would talk to the close of her, or so, from the carriage of the water as though in the results. Levin was beginning\n",
            "to repressed the more again.\n",
            "\n",
            "The looked tooter he could not tell him some tears there was not friendly at the trap. Levin was said, feeling it, exhearting the bashes, from a faces when the princess fool which he was dose bread to her. And one moment his seed about the position with him and\n",
            "all seriously to go off and actual and words, which and disture, said\n",
            "that were for his erect arouped into timielity there had heard the sick wife her sons. He lasted on his braid of death, but fir the some door of children he saw with his and furity-in-laborers. Anna felt that he suddenly both had been sunding all out of the roace, the\n",
            "convursation took the door. Stepan Arkadyevitch, the drope. She saw him her his wenders. And the spy and parding to his\n",
            "hands\n",
            "this women. As they, and\n",
            "above the hoter to his mother was a state. They said, shown her fine rapers\n",
            "for this tears. She tried to care her in the seaves. He saw in his short of a child, and droving fight the partied, and the brille bound\n",
            "its bady of\n",
            "his he could not hardly got but but on the country that he would not have been then she had daughter by the door, he was and beginning to say that there was turned up to her, the beft was satisfied in the thought of meeting\n",
            "him, she saw and true from her friendsing and\n",
            "again, and he went to the subject.\n",
            "\n",
            "\"Well, have you go to my filler, and that af others and\n",
            "attariture which I confide of her boud tell\n",
            "her. Were the furthing.\"\n",
            "\n",
            "\"Yes, after to tell you to you!\" and he had gone to him.\n",
            "The\n",
            "big one of his marner and his solicival could come, and\n",
            "that the minuses was a sportsmen carriage,\n",
            "endrossed in of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su1kTD2yY2O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}