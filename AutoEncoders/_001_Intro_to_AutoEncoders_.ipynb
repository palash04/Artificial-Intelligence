{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_001_Intro_to_AutoEncoders_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cQM7975Tjqz"
      },
      "source": [
        "# Intro to AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoCUoUMuCPU"
      },
      "source": [
        "**AutoEncoders:**\n",
        "\n",
        "---\n",
        "\n",
        "![Screenshot 2021-02-24 at 09 23 17](https://user-images.githubusercontent.com/26361028/108945061-efbe0a80-7681-11eb-9781-5c281f5cdb02.png)\n",
        "\n",
        "*   Aims to have outputs identical to inputs\n",
        "*   Not a pure type of unsupervised deep learning algorithm, but a self supervised (generate their own labels from the training data) deep learning algorithm, since we are comparing outputs to certain values, which are the inputs.\n",
        "*   So we have input, they get encoded, and then decoded to get outputs similar to inputs.\n",
        "*  Use cases-\n",
        "      - For Feature detection. Since encoded nodes represents those features.\n",
        "      - For building powerful recommender systems.\n",
        "      - For encoding data. Millions of data in the input can be encoded into a smaller representation. So then we can just make use of encoded data to decode outputs. Adv: Takes up less space.\n",
        "\n",
        "**Example of AutoEncoders:**\n",
        "\n",
        "---\n",
        "\n",
        "![Screenshot 2021-02-24 at 09 38 58](https://user-images.githubusercontent.com/26361028/108946287-2006a880-7684-11eb-8350-693272a8ffe9.png)\n",
        "\n",
        "---\n",
        "\n",
        "![Screenshot 2021-02-24 at 09 42 54](https://user-images.githubusercontent.com/26361028/108946552-ad49fd00-7684-11eb-89ab-5c03ff4b7625.png)\n",
        "\n",
        "---\n",
        "\n",
        "![Screenshot 2021-02-24 at 09 44 42](https://user-images.githubusercontent.com/26361028/108946681-ed10e480-7684-11eb-806d-a7395349c6f3.png)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Additional Reading-\n",
        "https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NccUvmlnStVc"
      },
      "source": [
        "**AutoEncoders with biases:**\n",
        "\n",
        "---\n",
        "\n",
        "b = bias_term\n",
        "![Screenshot 2021-02-24 at 09 57 11](https://user-images.githubusercontent.com/26361028/108947538-ab813900-7686-11eb-91bd-17bc276a47ab.png)\n",
        "\n",
        "Similar representation-\n",
        "![Screenshot 2021-02-24 at 09 57 26](https://user-images.githubusercontent.com/26361028/108947557-b4720a80-7686-11eb-932c-424e0a889670.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyansTbiTZ7g"
      },
      "source": [
        "# Training an AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnjiE2DwToXC"
      },
      "source": [
        "![Screenshot 2021-02-24 at 09 59 43](https://user-images.githubusercontent.com/26361028/108947707-061a9500-7687-11eb-99a5-467f60fe2beb.png)\n",
        "\n",
        "We will be using movies dataset.\n",
        "\n",
        "**Steps** for training an auto-encoder:\n",
        "1. We start with an array where the lines(the observations) correspond to the user, and the columns (the features) correspond to the movies. Each cell `(u,i)` contains the rating (from 1 to 5, 0 if no rating) of the movie `i` by user `u`.\n",
        "2. The first user goes into the network. The input vector `x = (r_1,r_2,...r_m)` contains all its rating for the movies.\n",
        "3. The inputs vector `x` is encoded into a vector of `z` of lower dimensions by mapping a function `f` (ex. sigmoid):\n",
        "    `z = f(Wx + b)` where W is the vector of input weights and b is the bias.\n",
        "4. `z` is then decoded to the output vector `y` of same dimensions as `x`, aiming to replicate the input `x`.\n",
        "5. The reconstruction error `d(x,y) = ||x-y||` is then computed. The goal is to minimize it.\n",
        "6. Backpropagation step is carried to update the weights depending on how much they are responsible for the error by use of learning rate.\n",
        "7. Repeat steps `1 to 6` and update the weights after each observation `(Reinforcement Learning)`. OR  Repeat steps `1 to 6` and update the weights only after a batch of observations `(Batch Learning)`.\n",
        "8. When the whole training process is passed through the Feed Forward NN, that makes an epoch, redo more epochs.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Additional Reading: \n",
        "https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keVWBjDnt6JP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}